{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers_own import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import data -> use of the imported function made by the ML team (takes a long time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import load_csv_data\n",
    "\n",
    "x_train, x_test, y_train, train_ids, test_ids = load_csv_data(\"./dataset\", sub_sample=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method allows us to take a specific feature out of X. My idea is to take a few of the interesting features out and concatenate them together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_first_line(filename):\n",
    "    with open(filename, 'r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        first_line = next(reader)\n",
    "        return first_line\n",
    "        \n",
    "def extract_feature(name):\n",
    "    filename = './dataset/x_train.csv'\n",
    "    first_line = np.array(read_first_line(filename))\n",
    "    index = np.where(first_line == name)\n",
    "    ind = index[0].item()\n",
    "    return x_train.copy()[:, ind-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a quick method to see that our data is not \"clean\". There are a lot of nan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def caracteristics(x, y):\n",
    "    count = np.sum(~np.isnan(x))\n",
    "    print(\"Number of x features beeing not nan:\", count)\n",
    "    nan_count = np.sum(np.isnan(x))\n",
    "    print(\"Number of x features beeing nan:\", nan_count)\n",
    "    num_negatives = np.sum(y == -1)\n",
    "    print(\"Number of y -1s:\", num_negatives)\n",
    "    num_positives = np.sum(y == 1)\n",
    "    print(\"Number of y 1s:\", num_positives)\n",
    "    num_null = np.sum(y == 0)\n",
    "    print(\"Number of y 0s:\", num_null)\n",
    "    nan_count_y = np.sum(np.isnan(y))\n",
    "    print(\"Number of y nan:\", nan_count_y)\n",
    "\n",
    "#Here is for the specific feature \"_BMI5\"\n",
    "caracteristics(x_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to extract a few features and replace the lack of data with the mean (those features are taken from the website: https://medium.com/@alexteboul17/building-predictive-models-for-heart-disease-using-the-2015-behavioral-risk-factor-surveillance-b786368021ab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing = extract_feature('_RFCHOL')\n",
    "np.unique(testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_mean(x):\n",
    "    mean_value = np.nanmean(x)\n",
    "    x_new = x.copy()\n",
    "    x_new[np.isnan(x_new)] = mean_value\n",
    "    return x_new\n",
    "\n",
    "def replace_mode(x):\n",
    "    unique, counts = np.unique(x[~np.isnan(x)], return_counts=True)\n",
    "    mode_value = unique[np.argmax(counts)]\n",
    "    # Step 2: Replace NaN values with the mode\n",
    "    x[np.isnan(x)] = mode_value\n",
    "    return x\n",
    "\n",
    "\n",
    "################Body mass idex - continuous feature\n",
    "_BMI5 = extract_feature('_BMI5')\n",
    "_BMI5 = replace_mean(_BMI5)\n",
    "_BMI5, _BMI5_mean, _BMI5_std = standardize(_BMI5)\n",
    "\n",
    "\n",
    "################High blood pressure - categorical feature (1 = no, 2 = yes, 9 = missing)\n",
    "_RFHYPE5 = extract_feature('_RFHYPE5')\n",
    "_RFHYPE5[_RFHYPE5 == 9] = np.nan\n",
    "_RFHYPE5[_RFHYPE5 == 1] = 0\n",
    "_RFHYPE5[_RFHYPE5 == 2] = 1\n",
    "_RFHYPE5 = replace_mode(_RFHYPE5)\n",
    "\n",
    "################High cholesterol - categorical feature (1 = no, 2 = yes, 9 = missing)\n",
    "_RFCHOL = extract_feature('_RFCHOL')\n",
    "_RFCHOL[_RFCHOL == 9] = np.nan\n",
    "_RFCHOL[_RFCHOL == 1] = 0\n",
    "_RFCHOL[_RFCHOL == 2] = 1\n",
    "_RFCHOL = replace_mode(_RFCHOL)\n",
    "\n",
    "################Smoking status - categorical feature (1 = every day, 2 = some days, 3 = formerly, 4 = never, 9 = missing)\n",
    "_SMOKER3 = extract_feature('_SMOKER3')\n",
    "_SMOKER3[_SMOKER3 == 9] = np.nan\n",
    "_SMOKER3 = replace_mode(_SMOKER3)\n",
    "_SMOKER3, _SMOKER3_mean, _SMOKER3_std = standardize(_SMOKER3)\n",
    "\n",
    "################Has ever had a stroke  - categorical feature (1 = yes, 2 = no, 7 = don't know, 9 = missing)\n",
    "CVDSTRK3 = extract_feature('CVDSTRK3')\n",
    "CVDSTRK3[CVDSTRK3 == 9] = np.nan\n",
    "CVDSTRK3[CVDSTRK3 == 7] = np.nan\n",
    "CVDSTRK3[CVDSTRK3 == 2] = 0\n",
    "CVDSTRK3 = replace_mode(CVDSTRK3)\n",
    "\n",
    "################Cholesterol checked  - categorical feature (1 = within the last 5 years, 2 = more than 5 years ago, 3 = never, 9 = missing)\n",
    "_CHOLCHK = extract_feature('_CHOLCHK')\n",
    "_CHOLCHK[_CHOLCHK == 9] = np.nan\n",
    "_CHOLCHK = replace_mode(_CHOLCHK)\n",
    "_CHOLCHK,_ , _ = standardize(_CHOLCHK)\n",
    "\n",
    "################Has ever had diabetes  - categorical feature (1 = yes, 2 = yes*, 3 = no, 4 = no - pre-diabetes, 7 = don't know, 9 = missing)\n",
    "DIABETE3 = extract_feature('DIABETE3')\n",
    "DIABETE3[DIABETE3 == 9] = np.nan\n",
    "DIABETE3[DIABETE3 == 7] = np.nan\n",
    "DIABETE3[DIABETE3 == 3] = 0\n",
    "DIABETE3[DIABETE3 == 4] = 0\n",
    "DIABETE3[DIABETE3 == 2] = 1\n",
    "DIABETE3 = replace_mode(DIABETE3)\n",
    "\n",
    "################Physical activity index  - categorical feature (1 = highly active, 2 = active, 3 = insufficiently active, 4 = inactive, 9 = missing)\n",
    "_PACAT1 = extract_feature('_PACAT1')\n",
    "_PACAT1[_PACAT1 == 9] = np.nan\n",
    "_PACAT1 = replace_mode(_PACAT1)\n",
    "_PACAT1, _PACAT1_mean, _PACAT1_std = standardize(_PACAT1)\n",
    "\n",
    "################Total fruits consumed per day  - continuous feature (implied 2 dp)\n",
    "#_FRUTSUM = extract_feature('_FRUTSUM')\n",
    "\n",
    "################Total vegetables consumed per day  - continuous feature (implied 2 dp)\n",
    "#_VEGESUM = extract_feature('_VEGESUM')\n",
    "\n",
    "################Computed number of drinks of alcohol beverages per week  - continuous feature (99900 = missing)\n",
    "_DRNKWEK = extract_feature('_DRNKWEK')\n",
    "_DRNKWEK[_DRNKWEK == 99900] = np.nan\n",
    "_DRNKWEK = replace_mean(_DRNKWEK)\n",
    "_DRNKWEK, _DRNKWEK_mean, _DRNKWEK_std = standardize(_DRNKWEK)\n",
    "\n",
    "################Have any healthcare coverage  - categorical feature (1 = yes, 2 = no, 7 = don't know, 9 = missing)\n",
    "HLTHPLN1 = extract_feature('HLTHPLN1')\n",
    "HLTHPLN1[HLTHPLN1 == 9] = np.nan\n",
    "HLTHPLN1[HLTHPLN1 == 7] = np.nan\n",
    "HLTHPLN1[HLTHPLN1 == 2] = 0\n",
    "HLTHPLN1 = replace_mode(HLTHPLN1)\n",
    "\n",
    "################Could not see doctor because of cost  - categorical feature (1 = yes, 2 = no, 7 = don't know, 9 = missing)\n",
    "MEDCOST = extract_feature('MEDCOST')\n",
    "MEDCOST[MEDCOST == 9] = np.nan\n",
    "MEDCOST[MEDCOST == 7] = np.nan\n",
    "MEDCOST[MEDCOST == 2] = 0\n",
    "MEDCOST = replace_mode(MEDCOST)\n",
    "\n",
    "################General health status  - categorical feature (1 = excellent, 2 = very good, 3 = good, 4 = fair, 5 = poor, 7 = don't know, 9 = missing)\n",
    "GENHLTH = extract_feature('GENHLTH')\n",
    "GENHLTH[GENHLTH == 9] = np.nan\n",
    "GENHLTH[GENHLTH == 7] = np.nan\n",
    "GENHLTH = replace_mode(GENHLTH)\n",
    "GENHLTH, GENHLTH_mean, GENHLTH_std = standardize(GENHLTH)\n",
    "\n",
    "################Number of days mental health not good  - continuous feature (88 = none, 77 = don't know, 99 = refused)\n",
    "MENTHLTH = extract_feature('MENTHLTH')\n",
    "MENTHLTH[MENTHLTH == 88] = 0\n",
    "MENTHLTH[MENTHLTH == 77] = np.nan\n",
    "MENTHLTH[MENTHLTH == 99] = np.nan\n",
    "MENTHLTH = replace_mean(MENTHLTH)\n",
    "MENTHLTH, MENTHLTH_mean, MENTHLTH_std = standardize(MENTHLTH)\n",
    "\n",
    "################Number of days physical health not good  - continuous feature (88 = none, 77 = don't know, 99 = refused)\n",
    "PHYSHLTH = extract_feature('PHYSHLTH')\n",
    "PHYSHLTH[PHYSHLTH == 88] = 0\n",
    "PHYSHLTH[PHYSHLTH == 77] = np.nan\n",
    "PHYSHLTH[PHYSHLTH == 99] = np.nan\n",
    "PHYSHLTH = replace_mean(PHYSHLTH)\n",
    "PHYSHLTH, PHYSHLTH_mean, PHYSHLTH_std = standardize(PHYSHLTH)\n",
    "\n",
    "################Difficulty walking or climbing stairs - categorical feature (1 = yes, 2 = no, 7 = don't know, 9 = missing)\n",
    "DIFFWALK = extract_feature('DIFFWALK')\n",
    "DIFFWALK[DIFFWALK == 9] = np.nan\n",
    "DIFFWALK[DIFFWALK == 7] = np.nan\n",
    "DIFFWALK[DIFFWALK == 2] = 0\n",
    "DIFFWALK = replace_mode(DIFFWALK)\n",
    "\n",
    "################Sex - categorical feature (1 = male, 2 = female)\n",
    "SEX = extract_feature('SEX')\n",
    "SEX[SEX == 2] = 0\n",
    "SEX = replace_mode(SEX)\n",
    "\n",
    "################Age  - categorical feature (1 = 18-24, ... 13 = 80+, 14 = missing)\n",
    "_AGEG5YR = extract_feature('_AGEG5YR')\n",
    "_AGEG5YR[_AGEG5YR == 14] = np.nan\n",
    "_AGEG5YR = replace_mode(_AGEG5YR)\n",
    "_AGEG5YR, _AGEG5YR_mean, _AGEG5YR_std = standardize(_AGEG5YR)\n",
    "\n",
    "################Education  - categorical feature (1 = none, ... 6 = college grad, 9 = missing)\n",
    "EDUCA = extract_feature('EDUCA')\n",
    "EDUCA[EDUCA == 9] = np.nan\n",
    "EDUCA = replace_mode(EDUCA)\n",
    "EDUCA, EDUCA_mean, EDUCA_std = standardize(EDUCA)\n",
    "\n",
    "################Income level  - categorical feature (1 = low, ... 5 = high, 9 = missing)\n",
    "_INCOMG = extract_feature('_INCOMG')\n",
    "_INCOMG[_INCOMG == 9] = np.nan\n",
    "_INCOMG = replace_mode(_INCOMG)\n",
    "_INCOMG, _INCOMG_mean, _INCOMG_std = standardize(_INCOMG)\n",
    "\n",
    "################BMI x age\n",
    "BMIxAGE = _BMI5 * _AGEG5YR\n",
    "BMIxAGE, BMIxAGE_mean, BMIxAGE_std = standardize(BMIxAGE)\n",
    "\n",
    "################Age2\n",
    "AGE2 = _AGEG5YR ** 2\n",
    "AGE2, AGE2_mean, AGE2_std = standardize(AGE2)\n",
    "\n",
    "################Age3\n",
    "AGE3 = _AGEG5YR ** 3\n",
    "AGE3, AGE3_mean, AGE3_std = standardize(AGE3)\n",
    "\n",
    "################Drink2\n",
    "DRNK2 = _DRNKWEK ** 2\n",
    "DRNK2, DRNK2_mean, DRNK2_std = standardize(DRNK2)\n",
    "\n",
    "################Physhealth2\n",
    "PHYSHLTH2 = PHYSHLTH ** 2\n",
    "PHYSHLTH2, PHYSHLTH2_mean, PHYSHLTH2_std = standardize(PHYSHLTH2)\n",
    "\n",
    "################Menthealth2\n",
    "MENTHLTH2 = MENTHLTH ** 2\n",
    "MENTHLTH2, MENTHLTH2_mean, MENTHLTH2_std = standardize(MENTHLTH2)\n",
    "\n",
    "################CHOLxHYPE\n",
    "CHOLxHYPE = _RFCHOL * _RFHYPE5\n",
    "\n",
    "# TRIED TO ADD INTERACTIONS and POLYNOMIALS, DIDN'T SEEM TO IMPROVE THE MODEL\n",
    "\n",
    "#Here we stack the features together to have the our new X\n",
    "X = np.hstack((_BMI5.reshape(-1, 1), _RFHYPE5.reshape(-1, 1), _RFCHOL.reshape(-1, 1), _SMOKER3.reshape(-1, 1), CVDSTRK3.reshape(-1, 1), \n",
    "               _CHOLCHK.reshape(-1, 1), DIABETE3.reshape(-1, 1), _PACAT1.reshape(-1, 1), # _FRUTSUM.reshape(-1, 1), _VEGESUM.reshape(-1, 1), \n",
    "               _DRNKWEK.reshape(-1, 1), HLTHPLN1.reshape(-1, 1), MEDCOST.reshape(-1, 1), GENHLTH.reshape(-1, 1), MENTHLTH.reshape(-1, 1), PHYSHLTH.reshape(-1, 1), \n",
    "               DIFFWALK.reshape(-1, 1), SEX.reshape(-1, 1), _AGEG5YR.reshape(-1, 1), EDUCA.reshape(-1, 1), _INCOMG.reshape(-1, 1)))#,\n",
    "               #BMIxAGE.reshape(-1, 1), AGE2.reshape(-1, 1), AGE3.reshape(-1, 1), DRNK2.reshape(-1, 1), PHYSHLTH2.reshape(-1, 1), MENTHLTH2.reshape(-1, 1), CHOLxHYPE.reshape(-1, 1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(_RFCHOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caracteristics(X, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_count_per_column = np.sum(np.isnan(X), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_count_per_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I just wanted to use the functions used in the exercise sessions. So I change from -1 to 0 for y negative so that I can use them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change all the elements with -1 by 0\n",
    "y_train_working = y_train.copy()\n",
    "y_train_working[y_train_working == -1] = 0\n",
    "#Make y have the correct shape\n",
    "y_train_working = y_train_working.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_nan(X, y):\n",
    "    mask = ~np.isnan(X).any(axis=1)\n",
    "    X = X[mask]\n",
    "    y = y[mask]\n",
    "    return X, y\n",
    "\n",
    "X, y_train_working = drop_nan(X, y_train_working)\n",
    "caracteristics(X, y_train_working)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the data into train and val sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_val(x, y, k_fold, k):\n",
    "    k_indices = build_k_indices(y, k_fold) \n",
    "    te_indice = k_indices[k]\n",
    "    tr_indice = k_indices[~(np.arange(k_indices.shape[0]) == k)]\n",
    "    tr_indice = tr_indice.reshape(-1)\n",
    "    y_te = y[te_indice]\n",
    "    y_tr = y[tr_indice]\n",
    "    x_te = x[te_indice]\n",
    "    x_tr = x[tr_indice]\n",
    "    return x_tr, x_te, y_tr, y_te\n",
    "\n",
    "np.random.seed(42)\n",
    "indices = np.arange(X.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "X_shuffled = X[indices]\n",
    "y_train_working_shuffled = y_train_working[indices]\n",
    "\n",
    "X_train, X_val, Y_train, Y_val = split_train_val(X, y_train_working, 10, 9)\n",
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "print(Y_train.shape)\n",
    "print(Y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now balance the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_data(x, y):\n",
    "    #Balance the data by taking only a certain number of values in y=-1 s.t. the number of y=1 equals the number of y=-1\n",
    "    \n",
    "    indices_y_equals_1 = np.where(y == 1)[0]\n",
    "    indices_y_equals_0 = np.where(y == 0)[0]\n",
    "    num_positives = np.sum(y == 1)\n",
    "    num_negatives = np.sum(y == 0)\n",
    "    selected_indices_neg = np.random.choice(indices_y_equals_0, size=num_positives, replace=False)\n",
    "    \n",
    "    selected_indices = np.concatenate((selected_indices_neg, indices_y_equals_1))\n",
    "    selected_X = x[selected_indices]\n",
    "    selected_y = y[selected_indices]\n",
    "    \n",
    "    return selected_X, selected_y\n",
    "\n",
    "def balance_data_multiplier(X, y): \n",
    "    class_1 = X[y == 1]\n",
    "    class_0 = X[y == 0]\n",
    "    count_class_1 = class_1.shape[0]\n",
    "    count_class_0 = class_0.shape[0]\n",
    "    num_to_duplicate = count_class_0 - count_class_1\n",
    "    duplicated_samples = np.tile(class_1, (num_to_duplicate // count_class_1 + 1, 1))[:num_to_duplicate]\n",
    "    X_balanced = np.vstack((X, duplicated_samples))\n",
    "    y_balanced = np.hstack((y, np.ones(num_to_duplicate)))\n",
    "    return X_balanced, y_balanced.reshape(-1, 1)\n",
    "\n",
    "def undersampling_oversampling(X, y, ratio_majority=0.5, ratio_majority_to_minority=1):\n",
    "    # Undersample the majority class\n",
    "    indices_y_equals_1 = np.where(y == 1)[0]\n",
    "    indices_y_equals_0 = np.where(y == 0)[0]\n",
    "    num_positives = np.sum(y == 1)\n",
    "    num_negatives = np.sum(y == 0)\n",
    "    selected_indices_neg = np.random.choice(indices_y_equals_0, size=int(np.floor(ratio_majority*num_negatives)), replace=False)\n",
    "    selected_indices = np.concatenate((selected_indices_neg, indices_y_equals_1))\n",
    "    selected_X = X[selected_indices]\n",
    "    selected_y = y[selected_indices]\n",
    "\n",
    "    # Oversample the minority class\n",
    "    num_to_duplicate = int(np.floor(ratio_majority*num_negatives) / ratio_majority_to_minority) - num_positives\n",
    "    duplicated_samples = np.tile(X[y == 1], (num_to_duplicate // num_positives + 1, 1))[:num_to_duplicate]\n",
    "    X_balanced = np.vstack((selected_X, duplicated_samples))\n",
    "    y_balanced = np.hstack((selected_y, np.ones(num_to_duplicate)))\n",
    "\n",
    "    # Shuffle the data\n",
    "    np.random.seed(42)\n",
    "    indices = np.arange(X_balanced.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    X_balanced = X_balanced[indices]\n",
    "    y_balanced = y_balanced[indices]\n",
    "    \n",
    "    return X_balanced, y_balanced.reshape(-1, 1)\n",
    "\n",
    "#print(y_train.shape)\n",
    "#X_train_balanced, Y_train_balanced = balance_data_multiplier(X_train, Y_train.reshape(-1))\n",
    "X_train_balanced, Y_train_balanced = undersampling_oversampling(X_train, Y_train.reshape(-1))\n",
    "caracteristics(X_train_balanced, Y_train_balanced)\n",
    "#X_balanced, y_balanced = balance_data(X, y_train)\n",
    "#caracteristics(X_balanced, y_balanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_gradient_descent(y_train, x_train, y_val, x_val, lambda1, lambda2, gamma):\n",
    "    # init parameters\n",
    "    max_iter = 10000\n",
    "    threshold = 1e-7\n",
    "    losses = []\n",
    "    losses_val = []\n",
    "\n",
    "    # build tx_train\n",
    "    tx_train = np.c_[np.ones((y_train.shape[0], 1)), x_train]\n",
    "    w = np.zeros((tx_train.shape[1], 1))\n",
    "    #print(tx_train)\n",
    "    #print(y_train)\n",
    "    tx_val = np.c_[np.ones((y_val.shape[0], 1)), x_val]\n",
    "\n",
    "    # start the logistic regression\n",
    "    for iter in range(max_iter):\n",
    "        # get loss and update w.\n",
    "        loss, w = learning_by_gradient_descent_ridge_lasso(y_train, tx_train, w, gamma, lambda1, lambda2)\n",
    "        loss_val = calculate_loss(y_val, tx_val, w)\n",
    "        # log info\n",
    "        if iter % 100 == 0:\n",
    "            print(\"Current iteration={i}, loss={l}\".format(i=iter, l=loss))\n",
    "        # converge criterion\n",
    "        losses.append(loss)\n",
    "        losses_val.append(loss_val)\n",
    "        if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "            print('finished')\n",
    "            break\n",
    "    return w, loss, losses, losses_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gamma = 0.01\n",
    "lambda1 = 0\n",
    "lambda2 = 0\n",
    "\n",
    "w, loss, losses, losses_val = logistic_regression_gradient_descent(Y_train_balanced, X_train_balanced, Y_val, X_val, lambda1, lambda2, gamma)\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the train and val losses\n",
    "plt.plot(losses, label='train')\n",
    "plt.plot(losses_val, label='val')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def prediction(tx_test, w):\n",
    "    compute = sigmoid(np.dot(tx_test, w))\n",
    "    y_test = (compute >= 0.7).astype(int)\n",
    "    return y_test\n",
    "#Now we test the result: % of well classified data\n",
    "def percentage_well_predicted(true_labels, predicted_labels):\n",
    "    # Check if both vectors have the same length\n",
    "    if len(true_labels) != len(predicted_labels):\n",
    "        raise ValueError(\"The two vectors must have the same length.\")\n",
    "    # Calculate the number of wrongly predicted points\n",
    "    num_right = np.sum(true_labels == predicted_labels)\n",
    "    # Calculate the percentage of wrongly predicted points\n",
    "    percentage_right = (num_right / len(true_labels)) * 100\n",
    "    return percentage_right\n",
    "tx_val = np.c_[np.ones((X_val.shape[0], 1)), X_val]\n",
    "print(tx_val.shape)\n",
    "print(w.shape)\n",
    "y_pred_test = prediction(tx_val, w)\n",
    "zero_count = np.sum(y_pred_test == 0)\n",
    "nonzero_count = np.sum(y_pred_test != 0)\n",
    "print(zero_count)\n",
    "print(nonzero_count)\n",
    "print(percentage_well_predicted(Y_val, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(y_pred, y_true):\n",
    "    tp = np.sum(y_pred[y_true == 1] == 1)\n",
    "    fp = np.sum(y_pred[y_true == 0] == 1)\n",
    "    fn = np.sum(y_pred[y_true == 1] == 0)\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    return f1\n",
    "\n",
    "print(f1(y_pred_test, Y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(y_pred, y_true):\n",
    "    tp = np.sum(y_pred[y_true == 1] == 1)\n",
    "    fp = np.sum(y_pred[y_true == 0] == 1)\n",
    "    fn = np.sum(y_pred[y_true == 1] == 0)\n",
    "    tn = np.sum(y_pred[y_true == 0] == 0)\n",
    "    return tp, fp, fn, tn\n",
    "\n",
    "tp, fp, fn, tn = confusion_matrix(y_pred_test, Y_val)\n",
    "print(tp)\n",
    "print(fp)\n",
    "print(fn)\n",
    "print(tn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(Y_val, return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to use our w to predict the results: we first format our x_test values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feature_test(name):\n",
    "    filename = './dataset/x_test.csv'\n",
    "    first_line = np.array(read_first_line(filename))\n",
    "    index = np.where(first_line == name)\n",
    "    ind = index[0].item()\n",
    "    return x_test.copy()[:, ind-1]\n",
    "\n",
    "################Body mass idex - continuous feature\n",
    "_BMI5 = extract_feature_test('_BMI5')\n",
    "_BMI5 = replace_mean(_BMI5)\n",
    "_BMI5, _BMI5_mean, _BMI5_std = standardize(_BMI5)\n",
    "\n",
    "\n",
    "################High blood pressure - categorical feature (1 = no, 2 = yes, 9 = missing)\n",
    "_RFHYPE5 = extract_feature_test('_RFHYPE5')\n",
    "_RFHYPE5[_RFHYPE5 == 9] = np.nan\n",
    "_RFHYPE5[_RFHYPE5 == 1] = 0\n",
    "_RFHYPE5[_RFHYPE5 == 2] = 1\n",
    "_RFHYPE5 = replace_mode(_RFHYPE5)\n",
    "\n",
    "################High cholesterol - categorical feature (1 = no, 2 = yes, 9 = missing)\n",
    "_RFCHOL = extract_feature_test('_RFCHOL')\n",
    "_RFCHOL[_RFCHOL == 9] = np.nan\n",
    "_RFCHOL[_RFCHOL == 1] = 0\n",
    "_RFCHOL[_RFCHOL == 2] = 1\n",
    "_RFCHOL = replace_mode(_RFCHOL)\n",
    "\n",
    "################Smoking status - categorical feature (1 = every day, 2 = some days, 3 = formerly, 4 = never, 9 = missing)\n",
    "_SMOKER3 = extract_feature_test('_SMOKER3')\n",
    "_SMOKER3[_SMOKER3 == 9] = np.nan\n",
    "_SMOKER3 = replace_mode(_SMOKER3)\n",
    "_SMOKER3, _SMOKER3_mean, _SMOKER3_std = standardize(_SMOKER3)\n",
    "\n",
    "################Has ever had a stroke  - categorical feature (1 = yes, 2 = no, 7 = don't know, 9 = missing)\n",
    "CVDSTRK3 = extract_feature_test('CVDSTRK3')\n",
    "CVDSTRK3[CVDSTRK3 == 9] = np.nan\n",
    "CVDSTRK3[CVDSTRK3 == 7] = np.nan\n",
    "CVDSTRK3[CVDSTRK3 == 2] = 0\n",
    "CVDSTRK3 = replace_mode(CVDSTRK3)\n",
    "\n",
    "################Cholesterol checked  - categorical feature (1 = within the last 5 years, 2 = more than 5 years ago, 3 = never, 9 = missing)\n",
    "_CHOLCHK = extract_feature_test('_CHOLCHK')\n",
    "_CHOLCHK[_CHOLCHK == 9] = np.nan\n",
    "_CHOLCHK = replace_mode(_CHOLCHK)\n",
    "_CHOLCHK,_ , _ = standardize(_CHOLCHK)\n",
    "\n",
    "################Has ever had diabetes  - categorical feature (1 = yes, 2 = yes*, 3 = no, 4 = no - pre-diabetes, 7 = don't know, 9 = missing)\n",
    "DIABETE3 = extract_feature_test('DIABETE3')\n",
    "DIABETE3[DIABETE3 == 9] = np.nan\n",
    "DIABETE3[DIABETE3 == 7] = np.nan\n",
    "DIABETE3[DIABETE3 == 3] = 0\n",
    "DIABETE3[DIABETE3 == 4] = 0\n",
    "DIABETE3[DIABETE3 == 2] = 1\n",
    "DIABETE3 = replace_mode(DIABETE3)\n",
    "\n",
    "################Physical activity index  - categorical feature (1 = highly active, 2 = active, 3 = insufficiently active, 4 = inactive, 9 = missing)\n",
    "_PACAT1 = extract_feature_test('_PACAT1')\n",
    "_PACAT1[_PACAT1 == 9] = np.nan\n",
    "_PACAT1 = replace_mode(_PACAT1)\n",
    "_PACAT1, _PACAT1_mean, _PACAT1_std = standardize(_PACAT1)\n",
    "\n",
    "################Total fruits consumed per day  - continuous feature (implied 2 dp)\n",
    "#_FRUTSUM = extract_feature('_FRUTSUM')\n",
    "\n",
    "################Total vegetables consumed per day  - continuous feature (implied 2 dp)\n",
    "#_VEGESUM = extract_feature('_VEGESUM')\n",
    "\n",
    "################Computed number of drinks of alcohol beverages per week  - continuous feature (99900 = missing)\n",
    "_DRNKWEK = extract_feature_test('_DRNKWEK')\n",
    "_DRNKWEK[_DRNKWEK == 99900] = np.nan\n",
    "_DRNKWEK = replace_mean(_DRNKWEK)\n",
    "_DRNKWEK, _DRNKWEK_mean, _DRNKWEK_std = standardize(_DRNKWEK)\n",
    "\n",
    "################Have any healthcare coverage  - categorical feature (1 = yes, 2 = no, 7 = don't know, 9 = missing)\n",
    "HLTHPLN1 = extract_feature_test('HLTHPLN1')\n",
    "HLTHPLN1[HLTHPLN1 == 9] = np.nan\n",
    "HLTHPLN1[HLTHPLN1 == 7] = np.nan\n",
    "HLTHPLN1[HLTHPLN1 == 2] = 0\n",
    "HLTHPLN1 = replace_mode(HLTHPLN1)\n",
    "\n",
    "################Could not see doctor because of cost  - categorical feature (1 = yes, 2 = no, 7 = don't know, 9 = missing)\n",
    "MEDCOST = extract_feature_test('MEDCOST')\n",
    "MEDCOST[MEDCOST == 9] = np.nan\n",
    "MEDCOST[MEDCOST == 7] = np.nan\n",
    "MEDCOST[MEDCOST == 2] = 0\n",
    "MEDCOST = replace_mode(MEDCOST)\n",
    "\n",
    "################General health status  - categorical feature (1 = excellent, 2 = very good, 3 = good, 4 = fair, 5 = poor, 7 = don't know, 9 = missing)\n",
    "GENHLTH = extract_feature_test('GENHLTH')\n",
    "GENHLTH[GENHLTH == 9] = np.nan\n",
    "GENHLTH[GENHLTH == 7] = np.nan\n",
    "GENHLTH = replace_mode(GENHLTH)\n",
    "GENHLTH, GENHLTH_mean, GENHLTH_std = standardize(GENHLTH)\n",
    "\n",
    "################Number of days mental health not good  - continuous feature (88 = none, 77 = don't know, 99 = refused)\n",
    "MENTHLTH = extract_feature_test('MENTHLTH')\n",
    "MENTHLTH[MENTHLTH == 88] = 0\n",
    "MENTHLTH[MENTHLTH == 77] = np.nan\n",
    "MENTHLTH[MENTHLTH == 99] = np.nan\n",
    "MENTHLTH = replace_mean(MENTHLTH)\n",
    "MENTHLTH, MENTHLTH_mean, MENTHLTH_std = standardize(MENTHLTH)\n",
    "\n",
    "################Number of days physical health not good  - continuous feature (88 = none, 77 = don't know, 99 = refused)\n",
    "PHYSHLTH = extract_feature_test('PHYSHLTH')\n",
    "PHYSHLTH[PHYSHLTH == 88] = 0\n",
    "PHYSHLTH[PHYSHLTH == 77] = np.nan\n",
    "PHYSHLTH[PHYSHLTH == 99] = np.nan\n",
    "PHYSHLTH = replace_mean(PHYSHLTH)\n",
    "PHYSHLTH, PHYSHLTH_mean, PHYSHLTH_std = standardize(PHYSHLTH)\n",
    "\n",
    "################Difficulty walking or climbing stairs - categorical feature (1 = yes, 2 = no, 7 = don't know, 9 = missing)\n",
    "DIFFWALK = extract_feature_test('DIFFWALK')\n",
    "DIFFWALK[DIFFWALK == 9] = np.nan\n",
    "DIFFWALK[DIFFWALK == 7] = np.nan\n",
    "DIFFWALK[DIFFWALK == 2] = 0\n",
    "DIFFWALK = replace_mode(DIFFWALK)\n",
    "\n",
    "################Sex - categorical feature (1 = male, 2 = female)\n",
    "SEX = extract_feature_test('SEX')\n",
    "SEX[SEX == 2] = 0\n",
    "SEX = replace_mode(SEX)\n",
    "\n",
    "################Age  - categorical feature (1 = 18-24, ... 13 = 80+, 14 = missing)\n",
    "_AGEG5YR = extract_feature_test('_AGEG5YR')\n",
    "_AGEG5YR[_AGEG5YR == 14] = np.nan\n",
    "_AGEG5YR = replace_mode(_AGEG5YR)\n",
    "_AGEG5YR, _AGEG5YR_mean, _AGEG5YR_std = standardize(_AGEG5YR)\n",
    "\n",
    "################Education  - categorical feature (1 = none, ... 6 = college grad, 9 = missing)\n",
    "EDUCA = extract_feature_test('EDUCA')\n",
    "EDUCA[EDUCA == 9] = np.nan\n",
    "EDUCA = replace_mode(EDUCA)\n",
    "EDUCA, EDUCA_mean, EDUCA_std = standardize(EDUCA)\n",
    "\n",
    "################Income level  - categorical feature (1 = low, ... 5 = high, 9 = missing)\n",
    "_INCOMG = extract_feature_test('_INCOMG')\n",
    "_INCOMG[_INCOMG == 9] = np.nan\n",
    "_INCOMG = replace_mode(_INCOMG)\n",
    "_INCOMG, _INCOMG_mean, _INCOMG_std = standardize(_INCOMG)\n",
    "\n",
    "#Here we stack the features together to have the our new X\n",
    "X_test = np.hstack((_BMI5.reshape(-1, 1), _RFHYPE5.reshape(-1, 1), _RFCHOL.reshape(-1, 1), _SMOKER3.reshape(-1, 1), CVDSTRK3.reshape(-1, 1), \n",
    "               _CHOLCHK.reshape(-1, 1), DIABETE3.reshape(-1, 1), _PACAT1.reshape(-1, 1), # _FRUTSUM.reshape(-1, 1), _VEGESUM.reshape(-1, 1), \n",
    "               _DRNKWEK.reshape(-1, 1), HLTHPLN1.reshape(-1, 1), MEDCOST.reshape(-1, 1), GENHLTH.reshape(-1, 1), MENTHLTH.reshape(-1, 1), PHYSHLTH.reshape(-1, 1), \n",
    "               DIFFWALK.reshape(-1, 1), SEX.reshape(-1, 1), _AGEG5YR.reshape(-1, 1), EDUCA.reshape(-1, 1), _INCOMG.reshape(-1, 1)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we actually want to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(tx_test, w):\n",
    "    compute = sigmoid(np.dot(tx_test, w))\n",
    "    y_test = (compute >= 0.7).astype(int)\n",
    "    return y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx_test = np.c_[np.ones((X_test.shape[0], 1)), X_test]\n",
    "print(tx_test)\n",
    "print(w)\n",
    "y_pred = prediction(tx_test, w)\n",
    "print(y_pred)\n",
    "nonzero_count = np.sum(y_pred != 0)\n",
    "zero_count = np.sum(y_pred == 0)\n",
    "print(nonzero_count)\n",
    "print(zero_count)\n",
    "y_pred[y_pred == 0] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import create_csv_submission\n",
    "create_csv_submission(test_ids, y_pred, \"Submission_7\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(y_pred, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
