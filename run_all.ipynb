{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers_create_data import *\n",
    "from implementations import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We import the data here\n",
    "from helpers import load_csv_data\n",
    "x_train, x_test, y_train, train_ids, test_ids = load_csv_data(\".\", sub_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we drop the rows that has a NaN percentage of {threshold} because we assume that they don't offer much information\n",
    "X_tr_all, Y_tr_all = drop_rows_with_nan(x_train, y_train, threshold=0.4)\n",
    "\n",
    "X_train_all, X_val_all, Y_train_all, Y_val_all = split_train_val(X_tr_all, Y_tr_all, 10, 9)\n",
    "\n",
    "# we process the dataset by replacing the remaining NaNs by column with the mode of the feature column that have less than 10 unique values and by its \n",
    "# mean if the feature column has more than 10 unique values. Also we remove the columns that have extremely low variance as this column \n",
    "# doesn't offer any information and we might encouter numerical issues when standardizing.\n",
    "X_tr_all, X_val_all, X_test_all = process_datasets(X_train_all, X_val_all, x_test, unique_values_thresh=10)\n",
    "\n",
    "# we replace the the -1s in the y column by 0s\n",
    "Y_train_all[Y_train_all == -1] = 0\n",
    "Y_val_all[Y_val_all == -1] = 0\n",
    "\n",
    "# We standardize the datasets in order to give us better numerical results\n",
    "X_tr_all, mean_x_tr_all, std_x_tr_all = standardize(X_tr_all)\n",
    "X_val_all,_,_ = standardize(X_val_all, mean_x_tr_all, std_x_tr_all)\n",
    "X_test_all,_,_ = standardize(X_test_all, mean_x_tr_all, std_x_tr_all)\n",
    "\n",
    "# We now balance the data to a slightly more balanced ratio of 0s and 1s\n",
    "X_tr_all, Y_tr_all = undersampling_oversampling(X_tr_all, Y_train_all, ratio_majority=0.5, ratio_majority_to_minority=2)\n",
    "\n",
    "# We add a column of ones (bias term) to the dataset\n",
    "X_tr_all = np.c_[np.ones((X_tr_all.shape[0], 1)), X_tr_all]\n",
    "X_val_all = np.c_[np.ones((X_val_all.shape[0], 1)), X_val_all]\n",
    "X_test_all = np.c_[np.ones((X_test_all.shape[0], 1)), X_test_all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gamma = 0.01 was giving us good results while converging in acceptable time\n",
    "gamma = 0.01\n",
    "max_iter = 10000\n",
    "\n",
    "#Reshape y_train form (#points,1) to (#points,) in order to use the implemented logistic regression function\n",
    "Y_tr_all = Y_tr_all.reshape(-1)\n",
    "#Create a new w in order to match the number of sected feature and has shape (1 + #features, )\n",
    "w_reg = np.zeros((X_tr_all.shape[1], 1)).reshape(-1)\n",
    "#Train model (-> our train set) using logistic regression\n",
    "w, loss = logistic_regression(Y_tr_all, X_tr_all, w_reg, max_iter, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 82.33387358184766\n",
      "F1:  0.4315514993481095\n"
     ]
    }
   ],
   "source": [
    "#y_pred_test are the predicted labels for the validation set\n",
    "y_pred_test = prediction(X_val_all, w)\n",
    "Y_val = Y_val_all.reshape(-1)\n",
    "print('Accuracy:', compute_accuracy(Y_val, y_pred_test))\n",
    "print('F1: ', f1(y_pred_test, Y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we actually make the prediction for the test set\n",
    "y_pred = prediction(X_test_all, w)\n",
    "y_pred[y_pred == 0] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import create_csv_submission\n",
    "create_csv_submission(test_ids, y_pred, \"Submission_25.10.2024_16_20\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
