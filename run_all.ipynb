{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers_create_data import *\n",
    "from implementations import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We import the data here\n",
    "from helpers import load_csv_data\n",
    "x_train, x_test, y_train, train_ids, test_ids = load_csv_data(\"./dataset\", sub_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change all the elements with -1 by 0\n",
    "y_train_working = y_train.copy()\n",
    "y_train_working[y_train_working == -1] = 0\n",
    "# Make y have the correct shape\n",
    "y_train_working = y_train_working.reshape(-1, 1)\n",
    "\n",
    "# Shuffle the data\n",
    "np.random.seed(6)\n",
    "indices = np.arange(x_train.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "X_shuffled = x_train[indices]\n",
    "y_train_working_shuffled = y_train_working[indices]\n",
    "\n",
    "# Split the data into training and validation sets (90% training, 10% validation)\n",
    "X_train_all, X_val_all, Y_train_all, Y_val_all = split_train_val(X_shuffled, y_train_working_shuffled, 10, 9)\n",
    "\n",
    "# we drop the rows that has a NaN percentage of {threshold} because we assume that they don't offer much information\n",
    "X_tr_all, Y_tr_all = drop_rows_with_nan(X_train_all, Y_train_all, threshold=0.4)\n",
    "\n",
    "# we process the dataset by replacing the remaining NaNs by column with the mode of the feature column that have less than 10 unique values and by its \n",
    "# mean if the feature column has more than 10 unique values. Also we remove the columns that have extremely low variance as this column \n",
    "# doesn't offer any information and we might encouter numerical issues when standardizing.\n",
    "X_tr_all, X_val_all, X_test_all = process_datasets(X_tr_all, X_val_all, x_test, unique_values_thresh=10)\n",
    "\n",
    "# We standardize the datasets in order to give us better numerical results\n",
    "# X_tr_all, mean_x_tr_all, std_x_tr_all = standardize(X_tr_all)\n",
    "# X_val_all,_,_ = standardize(X_val_all, mean_x_tr_all, std_x_tr_all)\n",
    "# X_test_all,_,_ = standardize(X_test_all, mean_x_tr_all, std_x_tr_all)\n",
    "\n",
    "# We now balance the data to a slightly more balanced ratio of 0s and 1s\n",
    "X_tr_all, Y_tr_all = undersampling_oversampling(X_tr_all, Y_tr_all, ratio_majority=0.5, ratio_majority_to_minority=2)\n",
    "\n",
    "# We add a column of ones (bias term) to the dataset\n",
    "X_tr_all = np.c_[np.ones((X_tr_all.shape[0], 1)), X_tr_all]\n",
    "X_val_all = np.c_[np.ones((X_val_all.shape[0], 1)), X_val_all]\n",
    "X_test_all = np.c_[np.ones((X_test_all.shape[0], 1)), X_test_all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gamma = 0.05 was giving us good results while converging in acceptable time\n",
    "gamma = 0.05\n",
    "max_iter = 10000\n",
    "\n",
    "#Reshape y_train form (#points,1) to (#points,) in order to use the implemented logistic regression function\n",
    "Y_tr_all = Y_tr_all.reshape(-1)\n",
    "#Create a new w in order to match the number of selected feature and has shape (1 + #features, )\n",
    "w_reg = np.zeros((X_tr_all.shape[1], 1)).reshape(-1)\n",
    "#Train model (-> our train set) using logistic regression\n",
    "w, loss = logistic_regression(Y_tr_all, X_tr_all, w_reg, max_iter, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 88.71483863103037\n",
      "F1:  0.42239900171580097\n"
     ]
    }
   ],
   "source": [
    "#y_pred_test are the predicted labels for the validation set\n",
    "y_pred_test = prediction(X_val_all, w)\n",
    "Y_val = Y_val_all.reshape(-1)\n",
    "print('Accuracy:', compute_accuracy(Y_val, y_pred_test))\n",
    "print('F1: ', f1(y_pred_test, Y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we actually make the prediction for the test set\n",
    "y_pred = prediction(X_test_all, w)\n",
    "y_pred[y_pred == 0] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import create_csv_submission\n",
    "create_csv_submission(test_ids, y_pred, \"Submission_27.10.2024_13_52\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
