{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfc6ecb3-b033-4086-8eb9-1eb0866cb6dd",
   "metadata": {},
   "source": [
    "### HERE IS OUR PARAMETERS VALIDATION FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b1a5e4ea-ef75-4b0b-8c82-a2a7f653438a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cedab778-839f-45e7-854b-44c7a97e44a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import *\n",
    "from helpers_create_data import *\n",
    "from implementations import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996fb883-81b3-43e1-9939-cc49e698bb7d",
   "metadata": {},
   "source": [
    "### LOADING THE CSV DATA INTO ARRAYS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ca4e3df-b779-42b8-9823-8a4ae8d13e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, train_ids, test_ids = load_csv_data(\"./dataset\", sub_sample=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df920e76-d60b-4920-ba48-bdc5b37249ff",
   "metadata": {},
   "source": [
    "#### FEATURE CHOICES :\n",
    "From the dataset, we filter the features in 3 different ways :\n",
    "\n",
    "1. Using (almost) all the features\n",
    "\n",
    "2. Choosing 19 features (taken mostly from https://medium.com/@alexteboul17/building-predictive-models-for-heart-disease-using-the-2015-behavioral-risk-factor-surveillance-b786368021ab)\n",
    "\n",
    "3. From the 19 features, we pick the 10 best ones using the best correlations towards the y variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a77498-c33c-4f55-b86d-32445524b4f3",
   "metadata": {},
   "source": [
    "### DATA PREPROCESSING\n",
    "\n",
    "#### 1. All features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a895b174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change all the elements with -1 by 0\n",
    "y_train_working = y_train.copy()\n",
    "y_train_working[y_train_working == -1] = 0\n",
    "# Make y have the correct shape\n",
    "y_train_working = y_train_working.reshape(-1, 1)\n",
    "\n",
    "# Shuffle the data\n",
    "np.random.seed(6)\n",
    "indices = np.arange(x_train.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "X_shuffled = x_train[indices]\n",
    "y_train_working_shuffled = y_train_working[indices]\n",
    "\n",
    "# Split the data into training and validation sets (90% training, 10% validation)\n",
    "X_train_all, X_val_all, Y_train_all, Y_val_all = split_train_val(X_shuffled, y_train_working_shuffled, 10, 9)\n",
    "\n",
    "# we drop the rows that has a NaN percentage of {threshold} because we assume that they don't offer much information\n",
    "X_tr_all, Y_tr_all = drop_rows_with_nan(X_train_all, Y_train_all, threshold=0.4)\n",
    "\n",
    "# we process the dataset by replacing the remaining NaNs by column with the mode of the feature column that have less than 10 unique values and by its \n",
    "# mean if the feature column has more than 10 unique values. Also we remove the columns that have extremely low variance as this column \n",
    "# doesn't offer any information and we might encouter numerical issues when standardizing.\n",
    "X_tr_all, X_val_all, X_test_all = process_datasets(X_tr_all, X_val_all, x_test, unique_values_thresh=10)\n",
    "\n",
    "# We now balance the data to a slightly more balanced ratio of 0s and 1s\n",
    "X_tr_all, Y_tr_all = undersampling_oversampling(X_tr_all, Y_tr_all, ratio_majority=0.5, ratio_majority_to_minority=2)\n",
    "\n",
    "# We add a column of ones (bias term) to the dataset\n",
    "X_tr_all = np.c_[np.ones((X_tr_all.shape[0], 1)), X_tr_all]\n",
    "X_val_all = np.c_[np.ones((X_val_all.shape[0], 1)), X_val_all]\n",
    "X_test_all = np.c_[np.ones((X_test_all.shape[0], 1)), X_test_all]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce8e464-d81e-4550-9b7b-b96ed129015c",
   "metadata": {},
   "source": [
    "#### 2. 19 Best Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "86d58b01-f0c9-441c-a065-531be39be6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We had to create the make_data function (-> helpers_create_data) because we manually preprocess \n",
    "# each feature in this case and do all the necessary further preprocessing within the function \n",
    "X_tr_19, Y_tr_19, X_val_19, Y_val_19, X_test_19 = make_data('./dataset/x_train.csv', './dataset/x_test.csv', x_train, x_test, y_train, replace=False)\n",
    "\n",
    "# Same as for all features, we re-balance the dataset \n",
    "X_tr_19, Y_tr_19 = undersampling_oversampling(X_tr_19, Y_tr_19, ratio_majority=1, ratio_majority_to_minority=2)\n",
    "\n",
    "# We add a column of ones (bias term) before training\n",
    "X_tr_19 = np.c_[np.ones((X_tr_19.shape[0], 1)), X_tr_19]\n",
    "X_val_19 = np.c_[np.ones((X_val_19.shape[0], 1)), X_val_19]\n",
    "X_test_19 = np.c_[np.ones((X_test_19.shape[0], 1)), X_test_19]\n",
    "\n",
    "#Reshape form (#points,1) to (#points,) in order to use the implemented logistic regression function\n",
    "Y_tr_19 = Y_tr_19.reshape(-1)\n",
    "Y_val_19 = Y_val_19.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c6fc5dbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(234226, 19)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tr_19[:, 1:].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81512fb6-45be-4f7e-a22c-ac18685c8500",
   "metadata": {},
   "source": [
    "#### 3. 10 Best Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8afa98c-b4a6-47ab-867a-cfb2db898940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We compute the correlations of all the feature variables with the output variable\n",
    "correlations = np.zeros(X_tr_19.shape[1])\n",
    "for i in range(len(correlations)):\n",
    "    if i == 0:\n",
    "        continue\n",
    "    else:\n",
    "        correlations[i] = np.abs(np.corrcoef(X_tr_19[:,i], Y_tr_19)[0,1])\n",
    "\n",
    "# We take the 10 biggest ones\n",
    "best_10_idx = np.argsort(correlations)[:-11:-1]\n",
    "\n",
    "# We use those 10 features for the dataset\n",
    "X_tr_10 = X_tr_19[:,best_10_idx]\n",
    "X_val_10 = X_val_19[:,best_10_idx]\n",
    "X_test_10 = X_test_19[:,best_10_idx]\n",
    "Y_tr_10 = Y_tr_19\n",
    "Y_val_10 = Y_val_19\n",
    "\n",
    "# We add a column of ones (bias term) before training\n",
    "X_tr_10 = np.c_[np.ones((X_tr_10.shape[0], 1)), X_tr_10]\n",
    "X_val_10 = np.c_[np.ones((X_val_10.shape[0], 1)), X_val_10]\n",
    "X_test_10 = np.c_[np.ones((X_test_10.shape[0], 1)), X_test_10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312a59e1-e4e3-4bf5-97d8-da45b7360451",
   "metadata": {},
   "source": [
    "### MODELS TRAINING\n",
    "\n",
    "#### 1. All features\n",
    "(takes a while to run...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33c251b-3067-47e8-bde0-883ba875bd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iter = 10000\n",
    "\n",
    "lambdas = np.logspace(-7,-1,7)\n",
    "gammas = np.array([0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1])\n",
    "\n",
    "# add 0 to lambdas to test the case without regularization\n",
    "lambdas = np.insert(lambdas, 0, 0)\n",
    "\n",
    "accuracies_all = np.zeros((8,7))\n",
    "f1_scores_all = np.zeros((8,7))\n",
    "\n",
    "ws_all = []\n",
    "losses_all = []\n",
    "\n",
    "for i, lambda_ in enumerate(lambdas):\n",
    "    for j, gamma in enumerate(gammas):\n",
    "        w_reg_all = np.zeros(X_tr_all.shape[1])\n",
    "        \n",
    "        #Reshape form (#points,1) to (#points,) in order to use the implemented logistic regression function\n",
    "        Y_tr_all = Y_tr_all.reshape(-1)\n",
    "        Y_val_all = Y_val_all.reshape(-1)\n",
    "\n",
    "        #Train model (-> our train set) using stocha logistic regression\n",
    "        w, loss = reg_logistic_regression(Y_tr_all, X_tr_all, lambda_, w_reg_all, max_iter, gamma)\n",
    "        Y_pred_all = prediction(X_val_all, w)\n",
    "\n",
    "        accuracies_all[i,j] = compute_accuracy(Y_val_all, Y_pred_all)\n",
    "        f1_scores_all[i,j] = f1(Y_pred_all, Y_val_all)\n",
    "        ws_all.append(w)\n",
    "        losses_all.append(loss)\n",
    "        \n",
    "best_idx_f1_all = np.unravel_index(np.argmax(f1_scores_all), f1_scores_all.shape)\n",
    "best_idx_acc_all = np.unravel_index(np.argmax(accuracies_all), accuracies_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "aeee0395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When using all features, the best index for accuracy is (5, 6) and the best one for f1 score is (0, 5) on the val set.\n",
      "When using all features, the best setting for f1 score is lambda = 0.0 and gamma = 0.05.\n",
      "When using all features, the best setting for accuracy is lambda = 0.001 and gamma = 0.1.\n",
      "\n",
      "When using all features, the best accuracy is 91.5399384390333 and the best f1 score is 0.42239900171580097 on the val set.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f'When using all features, the best index for accuracy is {best_idx_acc_all} and the best one for f1 score is {best_idx_f1_all} on the val set.')\n",
    "print(f'When using all features, the best setting for f1 score is lambda = {lambdas[best_idx_f1_all[0]]} and gamma = {gammas[best_idx_f1_all[1]]}.')\n",
    "print(f'When using all features, the best setting for accuracy is lambda = {lambdas[best_idx_acc_all[0]]} and gamma = {gammas[best_idx_acc_all[1]]}.')\n",
    "print(f'\\nWhen using all features, the best accuracy is {accuracies_all[best_idx_acc_all]} and the best f1 score is {f1_scores_all[best_idx_f1_all]} on the val set.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ad198a-03aa-40e8-b55c-645cc548c362",
   "metadata": {},
   "source": [
    "#### 2. 19 Features\n",
    "takes a looooong time... (please don't rerun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "25951bc6-29e9-4545-8053-1ecbb2cba0c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When using 19 features, the best index for accuracy is (0, 3) and the best one for f1 score is (1, 1) on the val set.\n",
      "When using 19 features, the best setting for f1 score is lambda = 1e-06 and gamma = 0.005.\n",
      "When using 19 features, the best setting for accuracy is lambda = 0.0 and gamma = 0.05.\n",
      "\n",
      "When using 19 features, the best accuracy is 85.31679517264499 and the best f1 score is 0.4114341558758856 on the val set.\n"
     ]
    }
   ],
   "source": [
    "max_iter = 10000\n",
    "\n",
    "lambdas = np.logspace(-6,-1,6)\n",
    "gammas = np.array([0.001, 0.005, 0.01, 0.05, 0.1])\n",
    "\n",
    "# add 0 to lambdas to test the case without regularization\n",
    "lambdas = np.insert(lambdas, 0, 0)\n",
    "\n",
    "accuracies_19 = np.zeros((7,5))\n",
    "f1_scores_19 = np.zeros((7,5))\n",
    "\n",
    "ws_19 = []\n",
    "losses_19 = []\n",
    "\n",
    "for i, lambda_ in enumerate(lambdas):\n",
    "    for j, gamma in enumerate(gammas):\n",
    "        # Initialize the weights\n",
    "        w_reg_19 = np.zeros(X_tr_19.shape[1])\n",
    "\n",
    "        #Train model (-> our train set) using stocha logistic regression\n",
    "        w, loss = reg_logistic_regression(Y_tr_19, X_tr_19, lambda_, w_reg_19, max_iter, gamma)\n",
    "        Y_pred_19 = prediction(X_val_19, w)\n",
    "\n",
    "        accuracies_19[i,j] = compute_accuracy(Y_val_19, Y_pred_19)\n",
    "        f1_scores_19[i,j] = f1(Y_pred_19, Y_val_19)\n",
    "        ws_19.append(w)\n",
    "        losses_19.append(loss)\n",
    "        \n",
    "best_idx_f1_19 = np.unravel_index(np.argmax(f1_scores_19), f1_scores_19.shape)\n",
    "best_idx_acc_19 = np.unravel_index(np.argmax(accuracies_19), accuracies_19.shape)\n",
    "\n",
    "print(f'When using 19 features, the best index for accuracy is {best_idx_acc_19} and the best one for f1 score is {best_idx_f1_19} on the val set.')\n",
    "print(f'When using 19 features, the best setting for f1 score is lambda = {lambdas[best_idx_f1_19[0]]} and gamma = {gammas[best_idx_f1_19[1]]}.')\n",
    "print(f'When using 19 features, the best setting for accuracy is lambda = {lambdas[best_idx_acc_19[0]]} and gamma = {gammas[best_idx_acc_19[1]]}.')\n",
    "print(f'\\nWhen using 19 features, the best accuracy is {accuracies_19[best_idx_acc_19]} and the best f1 score is {f1_scores_19[best_idx_acc_19]} on the val set.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "eee10ddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When using 19 features, the best index for accuracy is (1, 5) and the best one for f1 score is (2, 5) on the val set.\n",
      "When using 19 features, the best setting for f1 score is lambda = 1e-05 and gamma = 0.01.\n",
      "When using 19 features, the best setting for accuracy is lambda = 1e-06 and gamma = 0.01.\n",
      "\n",
      "When using 19 features, the best accuracy is 84.82613598268979 and the best f1 score is 0.406396779244017 on the val set.\n"
     ]
    }
   ],
   "source": [
    "print(f'When using 19 features, the best index for accuracy is {best_idx_acc_19} and the best one for f1 score is {best_idx_f1_19} on the val set.')\n",
    "print(f'When using 19 features, the best setting for f1 score is lambda = {lambdas[best_idx_f1_19[0]]} and gamma = {gammas[best_idx_f1_19[1]]}.')\n",
    "print(f'When using 19 features, the best setting for accuracy is lambda = {lambdas[best_idx_acc_19[0]]} and gamma = {gammas[best_idx_acc_19[1]]}.')\n",
    "print(f'\\nWhen using 19 features, the best accuracy is {accuracies_19[best_idx_acc_19]} and the best f1 score is {f1_scores_19[best_idx_f1_19]} on the val set.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2619d91-d09d-429f-909f-11c9b2ac5346",
   "metadata": {},
   "source": [
    "#### 3. 10 Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "8cd86a11-4b80-49d6-934e-e8d47b9b9b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When using 10 features, the best index for accuracy is (0, 0) and the best one for f1 score is (3, 1) on the val set.\n",
      "When using 10 features, the best setting for f1 score is lambda = 1e-06 and gamma = 0.005.\n",
      "When using 10 features, the best setting for accuracy is lambda = 0.0 and gamma = 0.05.\n",
      "\n",
      "When using 10 features, the best accuracy is 84.46042726967971 and the best f1 score is 0.39509954058192953 on the val set.\n"
     ]
    }
   ],
   "source": [
    "max_iter = 10000\n",
    "\n",
    "lambdas = np.logspace(-6,-1,6)\n",
    "gammas = np.array([0.001, 0.005, 0.01, 0.05, 0.1])\n",
    "\n",
    "# add 0 to lambdas to test the case without regularization\n",
    "lambdas = np.insert(lambdas, 0, 0)\n",
    "\n",
    "accuracies_10 = np.zeros((7,5))\n",
    "f1_scores_10 = np.zeros((7,5))\n",
    "\n",
    "ws_10 = []\n",
    "losses_10 = []\n",
    "\n",
    "for i, lambda_ in enumerate(lambdas):\n",
    "    for j, gamma in enumerate(gammas):\n",
    "        w_reg_10 = np.zeros(X_tr_10.shape[1])\n",
    "\n",
    "        #Train model (-> our train set) using stocha logistic regression\n",
    "        w, loss = reg_logistic_regression(Y_tr_10, X_tr_10, lambda_, w_reg_10, max_iter, gamma)\n",
    "        Y_pred_10 = prediction(X_val_10, w)\n",
    "\n",
    "        accuracies_10[i,j] = compute_accuracy(Y_val_10, Y_pred_10)\n",
    "        f1_scores_10[i,j] = f1(Y_pred_10, Y_val_10)\n",
    "        ws_10.append(w)\n",
    "        losses_10.append(loss)\n",
    "        \n",
    "best_idx_f1_10 = np.unravel_index(np.argmax(f1_scores_10), f1_scores_10.shape)\n",
    "best_idx_acc_10 = np.unravel_index(np.argmax(accuracies_10), accuracies_10.shape)\n",
    "\n",
    "# We pick our w according to the optimal f1_score\n",
    "#best_w_10 = ws_10[np.argmax(f1_scores_10)]\n",
    "\n",
    "print(f'When using 10 features, the best index for accuracy is {best_idx_acc_10} and the best one for f1 score is {best_idx_f1_10} on the val set.')\n",
    "print(f'When using 10 features, the best setting for f1 score is lambda = {lambdas[best_idx_f1_19[0]]} and gamma = {gammas[best_idx_f1_19[1]]}.')\n",
    "print(f'When using 10 features, the best setting for accuracy is lambda = {lambdas[best_idx_acc_19[0]]} and gamma = {gammas[best_idx_acc_19[1]]}.')\n",
    "print(f'\\nWhen using 10 features, the best accuracy is {accuracies_10[best_idx_acc_10]} and the best f1 score is {f1_scores_10[best_idx_f1_10]} on the val set.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5a24f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
