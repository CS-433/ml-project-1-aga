{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "b1a5e4ea-ef75-4b0b-8c82-a2a7f653438a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedab778-839f-45e7-854b-44c7a97e44a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import *\n",
    "from helpers_create_data import *\n",
    "from implementations import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996fb883-81b3-43e1-9939-cc49e698bb7d",
   "metadata": {},
   "source": [
    "### LOADING THE CSV DATA INTO ARRAYS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1ca4e3df-b779-42b8-9823-8a4ae8d13e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, train_ids, test_ids = load_csv_data(\".\", sub_sample=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df920e76-d60b-4920-ba48-bdc5b37249ff",
   "metadata": {},
   "source": [
    "#### FEATURE CHOICES :\n",
    "From the dataset, we filter the features in 3 different ways :\n",
    "\n",
    "1. Using (almost) all the features\n",
    "\n",
    "2. Choosing 19 features (taken mostly from https://medium.com/@alexteboul17/building-predictive-models-for-heart-disease-using-the-2015-behavioral-risk-factor-surveillance-b786368021ab)\n",
    "\n",
    "3. From the 19 features, we pick the 10 best ones using the best correlations towards the y variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a77498-c33c-4f55-b86d-32445524b4f3",
   "metadata": {},
   "source": [
    "### DATA PREPROCESSING\n",
    "\n",
    "#### 1. All features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "21eddc7c-cb5b-4f81-9e9b-3b0d444a5f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we drop the rows that has a NaN percentage of {threshold} because we assume that they don't offer much information\n",
    "X_tr_all, Y_tr_all = drop_rows_with_nan(x_train, y_train, threshold=0.4)\n",
    "\n",
    "# we process the dataset by replacing the remaining NaNs by column with the mode of the feature column that have less than 10 unique values and by its \n",
    "# mean if the feature column has more than 10 unique values. Also we remove the columns that have extremely low variance as this column \n",
    "# doesn't offer any information and we might encouter numerical issues when standardizing.\n",
    "X_tr_all, X_test_all = process_datasets(X_tr_all, x_test, unique_values_thresh=10)\n",
    "\n",
    "# we replace the the -1s in the y column by 0s\n",
    "Y_tr_all[Y_tr_all == -1] = 0\n",
    "\n",
    "# We standardize the datasets in order to give us better numerical results\n",
    "X_tr_all, mean_x_tr_all, std_x_tr_all = standardize(X_tr_all)\n",
    "X_test_all,_,_ = standardize(X_test_all, mean_x_tr_all, std_x_tr_all)\n",
    "\n",
    "# We now balance the data to a slightly more balanced ratio of 0s and 1s\n",
    "X_tr_all, Y_tr_all = undersampling_oversampling(X_tr_all, Y_tr_all, ratio_majority=0.5, ratio_majority_to_minority=2)\n",
    "\n",
    "# We add a column of ones (bias term) to the dataset\n",
    "X_tr_all = np.c_[np.ones((X_tr_all.shape[0], 1)), X_tr_all]\n",
    "X_test_all = np.c_[np.ones((X_test_all.shape[0], 1)), X_test_all]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce8e464-d81e-4550-9b7b-b96ed129015c",
   "metadata": {},
   "source": [
    "#### 2. 19 Best Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "86d58b01-f0c9-441c-a065-531be39be6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We had to create the make_data function (-> helpers_create_data) because we manually preprocess \n",
    "# each feature in this case and do all the necessary further preprocessing within the function \n",
    "X_tr_19, Y_tr_19, X_test_19 = make_data('./x_train.csv', './x_test.csv', x_train, x_test, y_train, replace=True)\n",
    "\n",
    "# Same as for all features, we re-balance the dataset \n",
    "X_tr_19, Y_tr_19 = undersampling_oversampling(X_tr_19, Y_tr_19, ratio_majority=1, ratio_majority_to_minority=2)\n",
    "\n",
    "# We add a column of ones (bias term) before training\n",
    "X_tr_19 = np.c_[np.ones((X_tr_19.shape[0], 1)), X_tr_19]\n",
    "X_test_19 = np.c_[np.ones((X_test_19.shape[0], 1)), X_test_19]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81512fb6-45be-4f7e-a22c-ac18685c8500",
   "metadata": {},
   "source": [
    "#### 3. 10 Best Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "id": "a8afa98c-b4a6-47ab-867a-cfb2db898940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We compute the correlations of all the feature variables with the output variable\n",
    "correlations = np.zeros(X_train_balanced.shape[1])\n",
    "for i in range(len(correlations)):\n",
    "    correlations[i] = np.abs(np.corrcoef(X_train_balanced[:,i], Y_train_balanced[:,0])[0,1])\n",
    "\n",
    "# We take the 10 biggest ones\n",
    "best_10_idx = np.argsort(correlations)[:-11:-1]\n",
    "\n",
    "# We use those 10 features for the dataset\n",
    "X_tr_10 = X_tr_19[:,best_10_idx]\n",
    "X_test_10 = X_test_19[:,best_10_idx]\n",
    "Y_tr_10 = Y_tr_19\n",
    "\n",
    "# We add a column of ones (bias term) before training\n",
    "X_tr_10 = np.c_[np.ones((X_tr_10.shape[0], 1)), X_tr_10]\n",
    "X_test_10 = np.c_[np.ones((X_test_10.shape[0], 1)), X_test_10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312a59e1-e4e3-4bf5-97d8-da45b7360451",
   "metadata": {},
   "source": [
    "### MODELS TRAINING\n",
    "\n",
    "#### 1. All features\n",
    "(takes a while to run...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "id": "f33c251b-3067-47e8-bde0-883ba875bd49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When using all features, the best index for accuracy is (0, 2) and the best one for f1 score is (2, 2).\n",
      "\n",
      "When using all features, the best accuracy is 73.20987654320987 and the best f1 score is 0.6444080294961082.\n"
     ]
    }
   ],
   "source": [
    "max_iter = 10000\n",
    "\n",
    "k_fold = 5\n",
    "\n",
    "lambdas = np.logspace(-7,0,k_fold)\n",
    "gammas = np.logspace(-7,0,k_fold)\n",
    "\n",
    "accuracies_all = np.zeros((k_fold,k_fold))\n",
    "f1_scores_all = np.zeros((k_fold,k_fold))\n",
    "\n",
    "ws_all = []\n",
    "losses_all = []\n",
    "\n",
    "w_reg_all = np.zeros(X_tr_all.shape[1])\n",
    "\n",
    "for i, lambda_ in enumerate(lambdas):\n",
    "    for j, gamma in enumerate(gammas):\n",
    "        # each iteration we pick a new training and a new val set\n",
    "        X_train, X_val, Y_train, Y_val = split_train_val(X_tr_all, Y_tr_all, k_fold, j)\n",
    "        \n",
    "        #Reshape form (#points,1) to (#points,) in order to use the implemented logistic regression function\n",
    "        Y_train = Y_train.reshape(-1)\n",
    "        Y_val = Y_val.reshape(-1)\n",
    "\n",
    "        #Train model (-> our train set) using stocha logistic regression\n",
    "        w, loss = reg_logistic_regression_stoch(Y_train, X_train, lambda_, w_reg_all, max_iter, gamma)\n",
    "        Y_pred = prediction(X_val, w)\n",
    "\n",
    "        accuracies_all[i,j] = compute_accuracy(Y_val, Y_pred)\n",
    "        f1_scores_all[i,j] = f1(Y_pred, Y_val)\n",
    "        ws_all.append(w)\n",
    "        losses_all.append(loss)\n",
    "        \n",
    "best_idx_f1_all = np.unravel_index(np.argmax(f1_scores_all), f1_scores_all.shape)\n",
    "best_idx_acc_all = np.unravel_index(np.argmax(accuracies_all), accuracies_all.shape)\n",
    "# We pick our w according to the optimal f1_score\n",
    "best_w_all = ws_all[np.argmax(f1_scores_all)]\n",
    "print(f'When using all features, the best index for accuracy is {best_idx_acc_all} and the best one for f1 score is {best_idx_f1_all} on the val set.')\n",
    "print(f'\\nWhen using all features, the best accuracy is {accuracies_all[best_idx_acc_all]} and the best f1 score is {f1_scores_all[best_idx_acc_all]} on the val set.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ad198a-03aa-40e8-b55c-645cc548c362",
   "metadata": {},
   "source": [
    "#### 2. 19 Features\n",
    "takes a looooong time... (please don't rerun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "id": "25951bc6-29e9-4545-8053-1ecbb2cba0c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When using 19 features, the best index for accuracy is (0, 3) and the best one for f1 score is (0, 2).\n",
      "\n",
      "When using 19 features, the best accuracy is 77.76886393011544 and the best f1 score is 0.6406598948202579.\n"
     ]
    }
   ],
   "source": [
    "max_iter = 10000\n",
    "\n",
    "k_fold = 5\n",
    "\n",
    "lambdas = np.logspace(-7,0,k_fold)\n",
    "gammas = np.logspace(-7,0,k_fold)\n",
    "\n",
    "accuracies_19 = np.zeros((k_fold,k_fold))\n",
    "f1_scores_19 = np.zeros((k_fold,k_fold))\n",
    "\n",
    "ws_19 = []\n",
    "losses_19 = []\n",
    "\n",
    "w_reg_19 = np.zeros(X_tr_19.shape[1])\n",
    "\n",
    "for i, lambda_ in enumerate(lambdas):\n",
    "    for j, gamma in enumerate(gammas):\n",
    "        # each iteration we pick a new training and a new val set\n",
    "        X_train, X_val, Y_train, Y_val = split_train_val(X_tr_19, Y_tr_19, k_fold, j)\n",
    "        \n",
    "        #Reshape form (#points,1) to (#points,) in order to use the implemented logistic regression function\n",
    "        Y_train = Y_train.reshape(-1)\n",
    "        Y_val = Y_val.reshape(-1)\n",
    "\n",
    "        #Train model (-> our train set) using stocha logistic regression\n",
    "        w, loss = reg_logistic_regression_stoch(Y_train, X_train, lambda_, w_reg_19, max_iter, gamma)\n",
    "        Y_pred = prediction(X_val, w)\n",
    "\n",
    "        accuracies_19[i,j] = compute_accuracy(Y_val, Y_pred)\n",
    "        f1_scores_19[i,j] = f1(Y_pred, Y_val)\n",
    "        ws_19.append(w)\n",
    "        losses_19.append(loss)\n",
    "        \n",
    "best_idx_f1_19 = np.unravel_index(np.argmax(f1_scores_19), f1_scores_19.shape)\n",
    "best_idx_acc_19 = np.unravel_index(np.argmax(accuracies_19), accuracies_19.shape)\n",
    "# We pick our w according to the optimal f1_score\n",
    "best_w_19 = ws_19[np.argmax(f1_scores_19)]\n",
    "print(f'When using 19 features, the best index for accuracy is {best_idx_acc_19} and the best one for f1 score is {best_idx_f1_19} on the val set.')\n",
    "print(f'\\nWhen using 19 features, the best accuracy is {accuracies_19[best_idx_acc_19]} and the best f1 score is {f1_scores_19[best_idx_acc_19]} on the val set.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2619d91-d09d-429f-909f-11c9b2ac5346",
   "metadata": {},
   "source": [
    "#### 3. 10 Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "id": "8cd86a11-4b80-49d6-934e-e8d47b9b9b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When using 10 features, the best index for accuracy is (0, 3) and the best one for f1 score is (0, 4) on the val set.\n",
      "\n",
      "When using 10 features, the best accuracy is 73.58715514551857 and the best f1 score is 0.5588372136303575 on the val set.\n"
     ]
    }
   ],
   "source": [
    "max_iter = 10000\n",
    "\n",
    "k_fold = 5\n",
    "\n",
    "lambdas = np.logspace(-7,0,k_fold)\n",
    "gammas = np.logspace(-7,0,k_fold)\n",
    "\n",
    "accuracies_10 = np.zeros((k_fold,k_fold))\n",
    "f1_scores_10 = np.zeros((k_fold,k_fold))\n",
    "\n",
    "ws_10 = []\n",
    "losses_10 = []\n",
    "\n",
    "w_reg_10 = np.zeros(X_tr_10.shape[1])\n",
    "\n",
    "for i, lambda_ in enumerate(lambdas):\n",
    "    for j, gamma in enumerate(gammas):\n",
    "        # each iteration we pick a new training and a new val set\n",
    "        X_train, X_val, Y_train, Y_val = split_train_val(X_tr_10, Y_tr_10, k_fold, j)\n",
    "        \n",
    "        #Reshape form (#points,1) to (#points,) in order to use the implemented logistic regression function\n",
    "        Y_train = Y_train.reshape(-1)\n",
    "        Y_val = Y_val.reshape(-1)\n",
    "\n",
    "        #Train model (-> our train set) using stocha logistic regression\n",
    "        w, loss = reg_logistic_regression_stoch(Y_train, X_train, lambda_, w_reg_10, max_iter, gamma)\n",
    "        Y_pred = prediction(X_val, w)\n",
    "\n",
    "        accuracies_10[i,j] = compute_accuracy(Y_val, Y_pred)\n",
    "        f1_scores_10[i,j] = f1(Y_pred, Y_val)\n",
    "        ws_10.append(w)\n",
    "        losses_10.append(loss)\n",
    "        \n",
    "best_idx_f1_10 = np.unravel_index(np.argmax(f1_scores_10), f1_scores_10.shape)\n",
    "best_idx_acc_10 = np.unravel_index(np.argmax(accuracies_10), accuracies_10.shape)\n",
    "# We pick our w according to the optimal f1_score\n",
    "best_w_10 = ws_10[np.argmax(f1_scores_10)]\n",
    "print(f'When using 10 features, the best index for accuracy is {best_idx_acc_10} and the best one for f1 score is {best_idx_f1_10} on the val set.')\n",
    "print(f'\\nWhen using 10 features, the best accuracy is {accuracies_10[best_idx_acc_10]} and the best f1 score is {f1_scores_10[best_idx_acc_10]} on the val set.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48f66e4-70e8-4491-bb9e-e0072ba79722",
   "metadata": {},
   "source": [
    "### GENERATING THE PREDICTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f5bcef-248a-418f-b90b-3d536e7e94c3",
   "metadata": {},
   "source": [
    "#### 1. All features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "id": "ec008a85-7216-4a91-bb86-41be4d7f3187",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test_all = prediction(X_test_all,best_w_all)\n",
    "Y_test_all[Y_test_all == 0] = -1\n",
    "create_csv_submission(test_ids, Y_test_all, \"Y_test_all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da742de8-10b7-4f24-b5e8-2d011fcde73b",
   "metadata": {},
   "source": [
    "#### 2. 19 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "id": "edb199ff-7be5-4937-ace5-7ab1276f4faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test_19 = prediction(X_test_19, best_w_19)\n",
    "Y_test_19[Y_test_19 == 0] = -1\n",
    "create_csv_submission(test_ids, Y_test_19, \"Y_test_19\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3140bce0-4061-4c48-ad95-ddfbdefeae33",
   "metadata": {},
   "source": [
    "#### 3. 10 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "id": "33f486fe-bc8c-4a65-ada2-48d9084752f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test_10 = prediction(X_test_10, best_w_10)\n",
    "Y_test_10[Y_test_10 == 0] = -1\n",
    "create_csv_submission(test_ids, Y_test_10, \"Y_test_10\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
