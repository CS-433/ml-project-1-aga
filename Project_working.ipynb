{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers_own import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import data -> use of the imported function made by the ML team (takes a long time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import load_csv_data\n",
    "\n",
    "x_train, x_test, y_train, train_ids, test_ids = load_csv_data(\"data/dataset\", sub_sample=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just a quick print to see the number of features and the number of data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "321\n",
      "328135\n"
     ]
    }
   ],
   "source": [
    "print(x_train[0].shape[0])\n",
    "print(x_train[:,0].shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I found it interesting to have an entire list of all the different features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Id' '_STATE' 'FMONTH' 'IDATE' 'IMONTH' 'IDAY' 'IYEAR' 'DISPCODE' 'SEQNO'\n",
      " '_PSU' 'CTELENUM' 'PVTRESD1' 'COLGHOUS' 'STATERES' 'CELLFON3' 'LADULT'\n",
      " 'NUMADULT' 'NUMMEN' 'NUMWOMEN' 'CTELNUM1' 'CELLFON2' 'CADULT' 'PVTRESD2'\n",
      " 'CCLGHOUS' 'CSTATE' 'LANDLINE' 'HHADULT' 'GENHLTH' 'PHYSHLTH' 'MENTHLTH'\n",
      " 'POORHLTH' 'HLTHPLN1' 'PERSDOC2' 'MEDCOST' 'CHECKUP1' 'BPHIGH4' 'BPMEDS'\n",
      " 'BLOODCHO' 'CHOLCHK' 'TOLDHI2' 'CVDSTRK3' 'ASTHMA3' 'ASTHNOW' 'CHCSCNCR'\n",
      " 'CHCOCNCR' 'CHCCOPD1' 'HAVARTH3' 'ADDEPEV2' 'CHCKIDNY' 'DIABETE3'\n",
      " 'DIABAGE2' 'SEX' 'MARITAL' 'EDUCA' 'RENTHOM1' 'NUMHHOL2' 'NUMPHON2'\n",
      " 'CPDEMO1' 'VETERAN3' 'EMPLOY1' 'CHILDREN' 'INCOME2' 'INTERNET' 'WEIGHT2'\n",
      " 'HEIGHT3' 'PREGNANT' 'QLACTLM2' 'USEEQUIP' 'BLIND' 'DECIDE' 'DIFFWALK'\n",
      " 'DIFFDRES' 'DIFFALON' 'SMOKE100' 'SMOKDAY2' 'STOPSMK2' 'LASTSMK2'\n",
      " 'USENOW3' 'ALCDAY5' 'AVEDRNK2' 'DRNK3GE5' 'MAXDRNKS' 'FRUITJU1' 'FRUIT1'\n",
      " 'FVBEANS' 'FVGREEN' 'FVORANG' 'VEGETAB1' 'EXERANY2' 'EXRACT11' 'EXEROFT1'\n",
      " 'EXERHMM1' 'EXRACT21' 'EXEROFT2' 'EXERHMM2' 'STRENGTH' 'LMTJOIN3'\n",
      " 'ARTHDIS2' 'ARTHSOCL' 'JOINPAIN' 'SEATBELT' 'FLUSHOT6' 'FLSHTMY2'\n",
      " 'IMFVPLAC' 'PNEUVAC3' 'HIVTST6' 'HIVTSTD3' 'WHRTST10' 'PDIABTST'\n",
      " 'PREDIAB1' 'INSULIN' 'BLDSUGAR' 'FEETCHK2' 'DOCTDIAB' 'CHKHEMO3'\n",
      " 'FEETCHK' 'EYEEXAM' 'DIABEYE' 'DIABEDU' 'CAREGIV1' 'CRGVREL1' 'CRGVLNG1'\n",
      " 'CRGVHRS1' 'CRGVPRB1' 'CRGVPERS' 'CRGVHOUS' 'CRGVMST2' 'CRGVEXPT'\n",
      " 'VIDFCLT2' 'VIREDIF3' 'VIPRFVS2' 'VINOCRE2' 'VIEYEXM2' 'VIINSUR2'\n",
      " 'VICTRCT4' 'VIGLUMA2' 'VIMACDG2' 'CIMEMLOS' 'CDHOUSE' 'CDASSIST' 'CDHELP'\n",
      " 'CDSOCIAL' 'CDDISCUS' 'WTCHSALT' 'LONGWTCH' 'DRADVISE' 'ASTHMAGE'\n",
      " 'ASATTACK' 'ASERVIST' 'ASDRVIST' 'ASRCHKUP' 'ASACTLIM' 'ASYMPTOM'\n",
      " 'ASNOSLEP' 'ASTHMED3' 'ASINHALR' 'HAREHAB1' 'STREHAB1' 'CVDASPRN'\n",
      " 'ASPUNSAF' 'RLIVPAIN' 'RDUCHART' 'RDUCSTRK' 'ARTTODAY' 'ARTHWGT'\n",
      " 'ARTHEXER' 'ARTHEDU' 'TETANUS' 'HPVADVC2' 'HPVADSHT' 'SHINGLE2' 'HADMAM'\n",
      " 'HOWLONG' 'HADPAP2' 'LASTPAP2' 'HPVTEST' 'HPLSTTST' 'HADHYST2' 'PROFEXAM'\n",
      " 'LENGEXAM' 'BLDSTOOL' 'LSTBLDS3' 'HADSIGM3' 'HADSGCO1' 'LASTSIG3'\n",
      " 'PCPSAAD2' 'PCPSADI1' 'PCPSARE1' 'PSATEST1' 'PSATIME' 'PCPSARS1'\n",
      " 'PCPSADE1' 'PCDMDECN' 'SCNTMNY1' 'SCNTMEL1' 'SCNTPAID' 'SCNTWRK1'\n",
      " 'SCNTLPAD' 'SCNTLWK1' 'SXORIENT' 'TRNSGNDR' 'RCSGENDR' 'RCSRLTN2'\n",
      " 'CASTHDX2' 'CASTHNO2' 'EMTSUPRT' 'LSATISFY' 'ADPLEASR' 'ADDOWN' 'ADSLEEP'\n",
      " 'ADENERGY' 'ADEAT1' 'ADFAIL' 'ADTHINK' 'ADMOVE' 'MISTMNT' 'ADANXEV'\n",
      " 'QSTVER' 'QSTLANG' 'MSCODE' '_STSTR' '_STRWT' '_RAWRAKE' '_WT2RAKE'\n",
      " '_CHISPNC' '_CRACE1' '_CPRACE' '_CLLCPWT' '_DUALUSE' '_DUALCOR' '_LLCPWT'\n",
      " '_RFHLTH' '_HCVU651' '_RFHYPE5' '_CHOLCHK' '_RFCHOL' '_LTASTH1'\n",
      " '_CASTHM1' '_ASTHMS1' '_DRDXAR1' '_PRACE1' '_MRACE1' '_HISPANC' '_RACE'\n",
      " '_RACEG21' '_RACEGR3' '_RACE_G1' '_AGEG5YR' '_AGE65YR' '_AGE80' '_AGE_G'\n",
      " 'HTIN4' 'HTM4' 'WTKG3' '_BMI5' '_BMI5CAT' '_RFBMI5' '_CHLDCNT' '_EDUCAG'\n",
      " '_INCOMG' '_SMOKER3' '_RFSMOK3' 'DRNKANY5' 'DROCDY3_' '_RFBING5'\n",
      " '_DRNKWEK' '_RFDRHV5' 'FTJUDA1_' 'FRUTDA1_' 'BEANDAY_' 'GRENDAY_'\n",
      " 'ORNGDAY_' 'VEGEDA1_' '_MISFRTN' '_MISVEGN' '_FRTRESP' '_VEGRESP'\n",
      " '_FRUTSUM' '_VEGESUM' '_FRTLT1' '_VEGLT1' '_FRT16' '_VEG23' '_FRUITEX'\n",
      " '_VEGETEX' '_TOTINDA' 'METVL11_' 'METVL21_' 'MAXVO2_' 'FC60_' 'ACTIN11_'\n",
      " 'ACTIN21_' 'PADUR1_' 'PADUR2_' 'PAFREQ1_' 'PAFREQ2_' '_MINAC11'\n",
      " '_MINAC21' 'STRFREQ_' 'PAMISS1_' 'PAMIN11_' 'PAMIN21_' 'PA1MIN_'\n",
      " 'PAVIG11_' 'PAVIG21_' 'PA1VIGM_' '_PACAT1' '_PAINDX1' '_PA150R2'\n",
      " '_PA300R2' '_PA30021' '_PASTRNG' '_PAREC1' '_PASTAE1' '_LMTACT1'\n",
      " '_LMTWRK1' '_LMTSCL1' '_RFSEAT2' '_RFSEAT3' '_FLSHOT6' '_PNEUMO2'\n",
      " '_AIDTST3']\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "def read_first_line(filename):\n",
    "    with open(filename, 'r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        first_line = next(reader)\n",
    "        return first_line\n",
    "\n",
    "filename = 'data/dataset/x_train.csv'\n",
    "first_line = np.array(read_first_line(filename))\n",
    "print(first_line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method allows us to take a specific feature out of X. My idea is to take a few of the interesting features out and concatenate them together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def extract_feature(name):\n",
    "    filename = 'data/dataset/x_train.csv'\n",
    "    first_line = np.array(read_first_line(filename))\n",
    "    index = np.where(first_line == name)\n",
    "    ind = index[0].item()\n",
    "    return x_train[:, ind-1]\n",
    "    \n",
    "#here is the plot for one of them \n",
    "x_extracted = extract_feature('_BMI5')\n",
    "plt.scatter(x_extracted, y_train, s=0.01)\n",
    "plt.title('_BMI5')\n",
    "plt.xlim(8, 80)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a quick method to see that our data is not \"clean\". There are a lot of nan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of x features beeing not nan: 301062\n",
      "Number of x features beeing nan: 27073\n",
      "Number of y -1s: 299160\n",
      "Number of y 1s: 28975\n",
      "Number of y nan: 0\n"
     ]
    }
   ],
   "source": [
    "def caracteristics(x, y):\n",
    "    count = np.sum(~np.isnan(x))\n",
    "    print(\"Number of x features beeing not nan:\", count)\n",
    "    nan_count = np.sum(np.isnan(x))\n",
    "    print(\"Number of x features beeing nan:\", nan_count)\n",
    "    num_negatives = np.sum(y == -1)\n",
    "    print(\"Number of y -1s:\", num_negatives)\n",
    "    num_positives = np.sum(y == 1)\n",
    "    print(\"Number of y 1s:\", num_positives)\n",
    "    nan_count_y = np.sum(np.isnan(y))\n",
    "    print(\"Number of y nan:\", nan_count_y)\n",
    "\n",
    "#Here is for the specific feature \"_BMI5\"\n",
    "caracteristics(x_extracted,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first approach is to replace them by the mean of the other values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The first case is for continuous feature\n",
    "def replace_mean(x):\n",
    "    mean_value = np.nanmean(x)\n",
    "    x_new = x.copy()\n",
    "    x_new[np.isnan(x_new)] = mean_value\n",
    "    return x_new\n",
    "#The second case is for discrete feature\n",
    "#\"values\" corresponds to the values that are not actual values but correspond to \"lack of data\"\n",
    "def replace_with_mean_excluding_values(x, values):\n",
    "    # Calculate the mean of x excluding values\n",
    "    mean_without_values = np.nanmean(np.where(np.isin(x, values), np.nan, x))\n",
    "    # Replace values in x with the calculated mean\n",
    "    x_new = np.where(np.isin(x, values) | np.isnan(x), mean_without_values, x)\n",
    "    return x_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to extract a few features and replace the lack of data with the mean (those features are taken from the website: https://medium.com/@alexteboul17/building-predictive-models-for-heart-disease-using-the-2015-behavioral-risk-factor-surveillance-b786368021ab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "_RFCHOL = extract_feature('_RFCHOL')\n",
    "_SMOKER3 = extract_feature('_SMOKER3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.41799901, -1.37501693, -0.33203485,  0.71094723,         nan])"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(_SMOKER3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "################Body mass idex - continuous feature\n",
    "_BMI5 = extract_feature('_BMI5')\n",
    "_BMI5, _BMI5_mean, _BMI5_std = standardize(_BMI5)\n",
    "\n",
    "################High blood pressure - categorical feature (1 = no, 2 = yes, 9 = missing)\n",
    "_RFHYPE5 = extract_feature('_RFHYPE5')\n",
    "_RFHYPE5[_RFHYPE5 == 9] = np.nan\n",
    "_RFHYPE5[_RFHYPE5 == 1] = 0\n",
    "_RFHYPE5[_RFHYPE5 == 2] = 1\n",
    "\n",
    "################High cholesterol - categorical feature (1 = no, 2 = yes, 9 = missing)\n",
    "_RFCHOL = extract_feature('_RFCHOL')\n",
    "_RFCHOL[_RFCHOL == 9] = np.nan\n",
    "_RFCHOL[_RFCHOL == 1] = 0\n",
    "_RFCHOL[_RFCHOL == 2] = 1\n",
    "\n",
    "################Smoking status - categorical feature (1 = every day, 2 = some days, 3 = formerly, 4 = never, 9 = missing)\n",
    "_SMOKER3 = extract_feature('_SMOKER3')\n",
    "_SMOKER3[_SMOKER3 == 9] = np.nan\n",
    "_SMOKER3, _SMOKER3_mean, _SMOKER3_std = standardize(_SMOKER3)\n",
    "\n",
    "################Has ever had a stroke  - categorical feature (1 = yes, 2 = no, 7 = don't know, 9 = missing)\n",
    "CVDSTRK3 = extract_feature('CVDSTRK3')\n",
    "CVDSTRK3[CVDSTRK3 == 9] = np.nan\n",
    "CVDSTRK3[CVDSTRK3 == 7] = np.nan\n",
    "CVDSTRK3[CVDSTRK3 == 2] = 0\n",
    "\n",
    "################Cholesterol checked  - categorical feature (1 = within the last 5 years, 2 = more than 5 years ago, 3 = never, 9 = missing)\n",
    "_CHOLCHK = extract_feature('_CHOLCHK')\n",
    "_CHOLCHK[_CHOLCHK == 9] = np.nan\n",
    "_CHOLCHK[_CHOLCHK == 3] = 0\n",
    "_CHOLCHK[_CHOLCHK == 2] = 0\n",
    "_CHOLCHK[_CHOLCHK == 1] = 1\n",
    "\n",
    "################Has ever had diabetes  - categorical feature (1 = yes, 2 = yes*, 3 = no, 4 = no - pre-diabetes, 7 = don't know, 9 = missing)\n",
    "DIABETE3 = extract_feature('DIABETE3')\n",
    "DIABETE3[DIABETE3 == 9] = np.nan\n",
    "DIABETE3[DIABETE3 == 7] = np.nan\n",
    "DIABETE3[DIABETE3 == 3] = 0\n",
    "DIABETE3[DIABETE3 == 4] = 0\n",
    "DIABETE3[DIABETE3 == 2] = 1\n",
    "\n",
    "################Physical activity index  - categorical feature (1 = highly active, 2 = active, 3 = insufficiently active, 4 = inactive, 9 = missing)\n",
    "_PACAT1 = extract_feature('_PACAT1')\n",
    "_PACAT1[_PACAT1 == 9] = np.nan\n",
    "_PACAT1, _PACAT1_mean, _PACAT1_std = standardize(_PACAT1)\n",
    "\n",
    "################Total fruits consumed per day  - continuous feature (implied 2 dp)\n",
    "#_FRUTSUM = extract_feature('_FRUTSUM')\n",
    "\n",
    "################Total vegetables consumed per day  - continuous feature (implied 2 dp)\n",
    "#_VEGESUM = extract_feature('_VEGESUM')\n",
    "\n",
    "################Computed number of drinks of alcohol beverages per week  - continuous feature (99900 = missing)\n",
    "_DRNKWEK = extract_feature('_DRNKWEK')\n",
    "_DRNKWEK[_DRNKWEK == 99900] = np.nan\n",
    "_DRNKWEK, _DRNKWEK_mean, _DRNKWEK_std = standardize(_DRNKWEK)\n",
    "\n",
    "################Have any healthcare coverage  - categorical feature (1 = yes, 2 = no, 7 = don't know, 9 = missing)\n",
    "HLTHPLN1 = extract_feature('HLTHPLN1')\n",
    "HLTHPLN1[HLTHPLN1 == 9] = np.nan\n",
    "HLTHPLN1[HLTHPLN1 == 7] = np.nan\n",
    "HLTHPLN1[HLTHPLN1 == 2] = 0\n",
    "\n",
    "################Could not see doctor because of cost  - categorical feature (1 = yes, 2 = no, 7 = don't know, 9 = missing)\n",
    "HLTHPLN1 = extract_feature('HLTHPLN1')\n",
    "HLTHPLN1[HLTHPLN1 == 9] = np.nan\n",
    "HLTHPLN1[HLTHPLN1 == 7] = np.nan\n",
    "HLTHPLN1[HLTHPLN1 == 2] = 0\n",
    "\n",
    "################General health status  - categorical feature (1 = excellent, 2 = very good, 3 = good, 4 = fair, 5 = poor, 7 = don't know, 9 = missing)\n",
    "GENHLTH = extract_feature('GENHLTH')\n",
    "GENHLTH[GENHLTH == 9] = np.nan\n",
    "GENHLTH[GENHLTH == 7] = np.nan\n",
    "GENHLTH, GENHLTH_mean, GENHLTH_std = standardize(GENHLTH)\n",
    "\n",
    "################Number of days mental health not good  - continuous feature (88 = none, 77 = don't know, 99 = refused)\n",
    "MENTHLTH = extract_feature('MENTHLTH')\n",
    "MENTHLTH[MENTHLTH == 88] = 0\n",
    "MENTHLTH[MENTHLTH == 77] = np.nan\n",
    "MENTHLTH[MENTHLTH == 99] = np.nan\n",
    "MENTHLTH, MENTHLTH_mean, MENTHLTH_std = standardize(MENTHLTH)\n",
    "\n",
    "################Number of days physical health not good  - continuous feature (88 = none, 77 = don't know, 99 = refused)\n",
    "PHYSHLTH = extract_feature('PHYSHLTH')\n",
    "PHYSHLTH[PHYSHLTH == 88] = 0\n",
    "PHYSHLTH[PHYSHLTH == 77] = np.nan\n",
    "PHYSHLTH[PHYSHLTH == 99] = np.nan\n",
    "PHYSHLTH, PHYSHLTH_mean, PHYSHLTH_std = standardize(PHYSHLTH)\n",
    "\n",
    "################Difficulty walking or climbing stairs - categorical feature (1 = yes, 2 = no, 7 = don't know, 9 = missing)\n",
    "DIFFWALK = extract_feature('DIFFWALK')\n",
    "DIFFWALK[DIFFWALK == 9] = np.nan\n",
    "DIFFWALK[DIFFWALK == 7] = np.nan\n",
    "DIFFWALK[DIFFWALK == 2] = 0\n",
    "\n",
    "################Sex - categorical feature (1 = male, 2 = female)\n",
    "SEX = extract_feature('SEX')\n",
    "SEX[SEX == 2] = 0\n",
    "\n",
    "################Age  - categorical feature (1 = 18-24, ... 13 = 80+, 14 = missing)\n",
    "_AGEG5YR = extract_feature('_AGEG5YR')\n",
    "_AGEG5YR[_AGEG5YR == 14] = np.nan\n",
    "_AGEG5YR, _AGEG5YR_mean, _AGEG5YR_std = standardize(_AGEG5YR)\n",
    "\n",
    "################Education  - categorical feature (1 = none, ... 6 = college grad, 9 = missing)\n",
    "EDUCA = extract_feature('EDUCA')\n",
    "EDUCA[EDUCA == 9] = np.nan\n",
    "EDUCA, EDUCA_mean, EDUCA_std = standardize(EDUCA)\n",
    "\n",
    "################Income level  - categorical feature (1 = low, ... 5 = high, 9 = missing)\n",
    "_INCOMG = extract_feature('_INCOMG')\n",
    "_INCOMG[_INCOMG == 9] = np.nan\n",
    "_INCOMG, _INCOMG_mean, _INCOMG_std = standardize(_INCOMG)\n",
    "\n",
    "#Here we stack the features together to have the our new X\n",
    "X = np.hstack((_BMI5.reshape(-1, 1), _RFHYPE5.reshape(-1, 1), _RFCHOL.reshape(-1, 1), _SMOKER3.reshape(-1, 1), CVDSTRK3.reshape(-1, 1), \n",
    "               _CHOLCHK.reshape(-1, 1), DIABETE3.reshape(-1, 1), _PACAT1.reshape(-1, 1), # _FRUTSUM.reshape(-1, 1), _VEGESUM.reshape(-1, 1), \n",
    "               _DRNKWEK.reshape(-1, 1), HLTHPLN1.reshape(-1, 1), GENHLTH.reshape(-1, 1), MENTHLTH.reshape(-1, 1), PHYSHLTH.reshape(-1, 1), \n",
    "               DIFFWALK.reshape(-1, 1), SEX.reshape(-1, 1), _AGEG5YR.reshape(-1, 1), EDUCA.reshape(-1, 1)))#, _INCOMG.reshape(-1, 1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of x features beeing not nan: 5387267\n",
      "Number of x features beeing nan: 191028\n",
      "Number of y -1s: 299160\n",
      "Number of y 1s: 28975\n",
      "Number of y nan: 0\n"
     ]
    }
   ],
   "source": [
    "caracteristics(X, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_count_per_column = np.sum(np.isnan(X), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([27073,   918, 46324, 13266,   737, 11245,   479, 41344, 18967,\n",
       "        1326,   888,  5359,  7108, 10779,     0,  3943,  1272])"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nan_count_per_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(328135,)"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I just wanted to use the functions used in the exercise sessions. So I change from -1 to 0 for y negative so that I can use them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change all the elements with -1 by 0\n",
    "y_train_working = y_train.copy()\n",
    "y_train_working[y_train_working == -1] = 0\n",
    "#Make y have the correct shape\n",
    "y_train_working = y_train_working.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the data into train and val sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "indices = np.arange(X.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "X = X[indices]\n",
    "y_train_working = y_train_working[indices]\n",
    "split_ratio = 0.9\n",
    "split_index = int(split_ratio * X.shape[0])\n",
    "X_train, X_val = X[:split_index], X[split_index:]\n",
    "y_train_working, y_val_working = y_train_working[:split_index], y_train_working[split_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_train = ~np.isnan(X_train).any(axis=1)\n",
    "X_train = X_train[mask_train]\n",
    "y_train_working = y_train_working[mask_train]\n",
    "\n",
    "mask_val = ~np.isnan(X_val).any(axis=1)\n",
    "X_val = X_val[mask_val]\n",
    "y_val_working = y_val_working[mask_val]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we see that the data is highly unbalanced (by a factor 10). We will sample randomly the \"0s\" so that there are as many as the \"1s\"\n",
    "This is probably stupid, but this first approach has the goal that we have a first working prediction that gives bad results and that we discuss it to improve it later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_data(x, y):\n",
    "    #Balance the data by taking only a certain number of values in y=-1 s.t. the number of y=1 equals the number of y=-1\n",
    "    \n",
    "    indices_y_equals_1 = np.where(y == 1)[0]\n",
    "    indices_y_equals_0 = np.where(y == 0)[0]\n",
    "    num_positives = np.sum(y == 1)\n",
    "    num_negatives = np.sum(y == 0)\n",
    "    selected_indices_neg = np.random.choice(indices_y_equals_0, size=num_positives, replace=False)\n",
    "    \n",
    "    selected_indices = np.concatenate((selected_indices_neg, indices_y_equals_1))\n",
    "    selected_X = x[selected_indices]\n",
    "    selected_y = y[selected_indices]\n",
    "    \n",
    "    return selected_X, selected_y\n",
    "\n",
    "#X_balanced, y_balanced = balance_data(X, y_train)\n",
    "#caracteristics(X_balanced, y_balanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we try logistic regression with gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_gradient_descent(y_train, x_train, lambda_, gamma, y_val = None, x_val = None, printing=False):\n",
    "    # init parameters\n",
    "    max_iter = 10000\n",
    "    threshold = 1e-8\n",
    "    losses = []\n",
    "\n",
    "    # build tx\n",
    "    tx_train = np.c_[np.ones((y_train.shape[0], 1)), x_train]\n",
    "    w = np.zeros((tx_train.shape[1], 1))\n",
    "    #print(tx)\n",
    "    #print(y)\n",
    "    if printing == True:\n",
    "        tx_val = np.c_[np.ones((y_val.shape[0], 1)), x_val]\n",
    "\n",
    "\n",
    "    # start the logistic regression\n",
    "    for iter in range(max_iter):\n",
    "        # get loss and update w.\n",
    "        loss, w = learning_by_gradient_descent_ridge(y_train, tx_train, w, gamma, lambda_)\n",
    "        # log info\n",
    "        if iter % 1000 == 0 and printing == True:\n",
    "            print(\"Current iteration={i}, train_loss={l}\".format(i=iter, l=loss))\n",
    "            print(\"Current iteration={i}, val_loss={l}\".format(i=iter, l=calculate_loss(y_val, tx_val, w)))\n",
    "        # converge criterion\n",
    "        losses.append(loss)\n",
    "        if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "            break\n",
    "    return w, loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, train_loss=0.6931471805599094\n",
      "Current iteration=0, val_loss=0.688784031218719\n",
      "Current iteration=1000, train_loss=0.2927080086550358\n",
      "Current iteration=1000, val_loss=0.293282979636002\n",
      "Current iteration=2000, train_loss=0.27644166725892216\n",
      "Current iteration=2000, val_loss=0.2763173966824201\n",
      "Current iteration=3000, train_loss=0.2686571624129299\n",
      "Current iteration=3000, val_loss=0.2680377883705867\n",
      "Current iteration=4000, train_loss=0.26395609951119703\n",
      "Current iteration=4000, val_loss=0.26296104427421724\n",
      "Current iteration=5000, train_loss=0.26080985578159405\n",
      "Current iteration=5000, val_loss=0.2595172826602631\n",
      "Current iteration=6000, train_loss=0.2585671820302219\n",
      "Current iteration=6000, val_loss=0.2570332778842315\n",
      "Current iteration=7000, train_loss=0.2568979468217969\n",
      "Current iteration=7000, val_loss=0.25516503653041905\n",
      "Current iteration=8000, train_loss=0.255615466627937\n",
      "Current iteration=8000, val_loss=0.2537163172052467\n",
      "Current iteration=9000, train_loss=0.2546056961126882\n",
      "Current iteration=9000, val_loss=0.252566165405824\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[-1.42076503],\n",
       "        [ 0.05156306],\n",
       "        [ 0.30799209],\n",
       "        [ 0.34179072],\n",
       "        [-0.15410666],\n",
       "        [ 0.43236324],\n",
       "        [-1.04348078],\n",
       "        [ 0.23927879],\n",
       "        [-0.0382996 ],\n",
       "        [-0.04171868],\n",
       "        [-0.98662942],\n",
       "        [ 0.56351237],\n",
       "        [ 0.00575629],\n",
       "        [ 0.05384919],\n",
       "        [ 0.13958552],\n",
       "        [ 0.32276404],\n",
       "        [ 0.87205497],\n",
       "        [-0.00348955]]),\n",
       " 0.2537955321156456)"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Small test if it's working\n",
    "lambda_ = 0.0001\n",
    "gamma = 0.008\n",
    "logistic_regression_gradient_descent(y_train=y_train_working, x_train=X_train, lambda_=lambda_, gamma=gamma, y_val=y_val_working, x_val=X_val, printing=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we try to do the same using the k_fold to guess the best lambda (gamma fixed for now -> I dont want to loop on every gamma yet because it would make the final program really slow to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(y, x, k_indices, k, lambda_, gamma):\n",
    "    \"\"\"return the loss of ridge regression for a fold corresponding to k_indices\n",
    "\n",
    "    Args:\n",
    "        y:          shape=(N,)\n",
    "        x:          shape=(N,)\n",
    "        k_indices:  2D array returned by build_k_indices()\n",
    "        k:          scalar, the k-th fold (N.B.: not to confused with k_fold which is the fold nums)\n",
    "        lambda_:    scalar, cf. ridge_regression()\n",
    "        degree:     scalar, cf. build_poly()\n",
    "\n",
    "    Returns:\n",
    "        train and test root mean square errors rmse = sqrt(2 mse)\n",
    "\n",
    "    >>> cross_validation(np.array([1.,2.,3.,4.]), np.array([6.,7.,8.,9.]), np.array([[3,2], [0,1]]), 1, 2, 3)\n",
    "    (0.019866645527597114, 0.33555914361295175)\n",
    "    \"\"\"\n",
    "    # get k'th subgroup in test, others in train\n",
    "    te_indice = k_indices[k]\n",
    "    tr_indice = k_indices[~(np.arange(k_indices.shape[0]) == k)]\n",
    "    tr_indice = tr_indice.reshape(-1)\n",
    "    y_te = y[te_indice]\n",
    "    y_tr = y[tr_indice]\n",
    "    x_te = x[te_indice]\n",
    "    x_tr = x[tr_indice]\n",
    "    \n",
    "    w, loss_tr = logistic_regression_gradient_descent(y_tr, x_tr, lambda_, gamma)\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # calculate the loss for train and test data: TODO\n",
    "    # ***************************************************\n",
    "    tx_te = np.c_[np.ones((y_te.shape[0], 1)), x_te]\n",
    "    loss_te = calculate_loss(y_te, tx_te, w)\n",
    "    \n",
    "    return w, loss_tr, loss_te"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the final function that call the other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_lambdas(y, x, k_fold, lambdas):\n",
    "    \"\"\"cross validation over regularisation parameter lambda.\n",
    "\n",
    "    Args:\n",
    "        k_fold: integer, the number of folds\n",
    "        lambdas: shape = (p, ) where p is the number of values of lambda to test\n",
    "    Returns:\n",
    "        best_lambda : scalar, value of the best lambda\n",
    "        best_loss : scalar, the associated root mean squared error for the best lambda\n",
    "        best_w\n",
    "    \"\"\"\n",
    "    k_fold = k_fold\n",
    "    lambdas = lambdas\n",
    "    gamma = 0.008\n",
    "    # split data in k fold\n",
    "    k_indices = build_k_indices(y, k_fold)\n",
    "    # define lists to store the loss of training data and test data\n",
    "    loss_tr = []\n",
    "    loss_te = []\n",
    "    # cross validation over lambdas\n",
    "    for lambda_ in lambdas:\n",
    "        print(lambda_)\n",
    "        loss_tr_tmp = []\n",
    "        loss_te_tmp = []\n",
    "        for k in range(k_fold):\n",
    "            print(k)\n",
    "            w, loss_tr_tmp_val, loss_te_tmp_val = cross_validation(y, x, k_indices, k, lambda_, gamma)\n",
    "            loss_tr_tmp.append(loss_tr_tmp_val)\n",
    "            loss_te_tmp.append(loss_te_tmp_val)\n",
    "        loss_tr.append(np.mean(loss_tr_tmp))  # Appending mean to the list\n",
    "        loss_te.append(np.mean(loss_te_tmp))  # Appending mean to the list\n",
    "        \n",
    "    best_loss = np.min(loss_te)\n",
    "    best_lambda = lambdas[np.argmin(loss_te)]\n",
    "    print(best_loss)\n",
    "    print(best_lambda)\n",
    "    return w, best_lambda, best_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10\n",
    "lambdas = np.array([0.00001, 0.0001, 0.001, 0.01, 0.1, 0.3, 0.5, 0.7, 0.9])\n",
    "cross_validation_lambdas(y_train_working, X_train, k, lambdas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, loss=0.6931471805599465\n",
      "Current iteration=1000, loss=0.5759364534909579\n",
      "Current iteration=2000, loss=0.5736866109878641\n",
      "Current iteration=3000, loss=0.5735058547358677\n",
      "Current iteration=0, loss=0.6931471805599465\n",
      "Current iteration=1000, loss=0.5769157352482289\n",
      "Current iteration=2000, loss=0.5746637560729676\n",
      "Current iteration=3000, loss=0.5744791942312685\n",
      "Current iteration=0, loss=0.6931471805599465\n",
      "Current iteration=1000, loss=0.5752434925589117\n",
      "Current iteration=2000, loss=0.5729165110626362\n",
      "Current iteration=3000, loss=0.5727230677676072\n",
      "Current iteration=0, loss=0.6931471805599465\n",
      "Current iteration=1000, loss=0.5756315845187144\n",
      "Current iteration=2000, loss=0.573358351878043\n",
      "Current iteration=3000, loss=0.5731755653502179\n",
      "Current iteration=0, loss=0.6931471805599465\n",
      "Current iteration=1000, loss=0.576594058698112\n",
      "Current iteration=2000, loss=0.5743020153365053\n",
      "Current iteration=3000, loss=0.5741110333104809\n",
      "Current iteration=0, loss=0.6931471805599466\n",
      "Current iteration=1000, loss=0.5756507314351238\n",
      "Current iteration=2000, loss=0.573349410384374\n",
      "Current iteration=3000, loss=0.5731591353783693\n",
      "Current iteration=0, loss=0.6931471805599465\n",
      "Current iteration=1000, loss=0.5761314494910668\n",
      "Current iteration=2000, loss=0.5738724407865473\n",
      "Current iteration=3000, loss=0.5736897443621413\n",
      "Current iteration=0, loss=0.6931471805599465\n",
      "Current iteration=1000, loss=0.5765308464290981\n",
      "Current iteration=2000, loss=0.5742714582757658\n",
      "Current iteration=3000, loss=0.5740844027736208\n",
      "Current iteration=0, loss=0.6931471805599465\n",
      "Current iteration=1000, loss=0.5762674897470396\n",
      "Current iteration=2000, loss=0.5739754049763913\n",
      "Current iteration=3000, loss=0.5737830458417776\n",
      "Current iteration=0, loss=0.6931471805599465\n",
      "Current iteration=1000, loss=0.5754835576717224\n",
      "Current iteration=2000, loss=0.5731513838026098\n",
      "Current iteration=3000, loss=0.5729535954749616\n",
      "Current iteration=4000, loss=0.5729254638449858\n",
      "[array([[ 0.02222243],\n",
      "       [ 0.03627152],\n",
      "       [ 0.64941213],\n",
      "       [-0.43318269],\n",
      "       [-0.21849709],\n",
      "       [-0.50369636]]), array([[ 0.02739649],\n",
      "       [ 0.03158801],\n",
      "       [ 0.64896069],\n",
      "       [-0.42660359],\n",
      "       [-0.21372333],\n",
      "       [-0.50622433]]), array([[ 0.02712314],\n",
      "       [ 0.03519462],\n",
      "       [ 0.65652741],\n",
      "       [-0.42659346],\n",
      "       [-0.21837082],\n",
      "       [-0.51281655]]), array([[ 0.01874159],\n",
      "       [ 0.03471529],\n",
      "       [ 0.65359898],\n",
      "       [-0.43223985],\n",
      "       [-0.21475112],\n",
      "       [-0.50515685]]), array([[ 0.02194715],\n",
      "       [ 0.034577  ],\n",
      "       [ 0.65081464],\n",
      "       [-0.42284466],\n",
      "       [-0.21647625],\n",
      "       [-0.51276426]]), array([[ 0.01880796],\n",
      "       [ 0.02897548],\n",
      "       [ 0.65173134],\n",
      "       [-0.43063653],\n",
      "       [-0.21335021],\n",
      "       [-0.51280107]]), array([[ 0.02425556],\n",
      "       [ 0.02973602],\n",
      "       [ 0.65220401],\n",
      "       [-0.4299797 ],\n",
      "       [-0.21162112],\n",
      "       [-0.50522526]]), array([[ 0.02489526],\n",
      "       [ 0.03916207],\n",
      "       [ 0.64630002],\n",
      "       [-0.42874338],\n",
      "       [-0.21170581],\n",
      "       [-0.5113581 ]]), array([[ 0.02191093],\n",
      "       [ 0.03731978],\n",
      "       [ 0.65193009],\n",
      "       [-0.42360451],\n",
      "       [-0.21217463],\n",
      "       [-0.51456764]]), array([[ 0.03156733],\n",
      "       [ 0.03358708],\n",
      "       [ 0.65140776],\n",
      "       [-0.42869996],\n",
      "       [-0.21606834],\n",
      "       [-0.51786372]])]\n",
      "[[ 0.02388678]\n",
      " [ 0.03411269]\n",
      " [ 0.65128871]\n",
      " [-0.42831283]\n",
      " [-0.21467387]\n",
      " [-0.51024742]]\n"
     ]
    }
   ],
   "source": [
    "gamma = 0.008\n",
    "lamda_ = 0.0001\n",
    "k_fold = 10\n",
    "# split data in k fold\n",
    "k_indices = build_k_indices(y_final, k_fold)\n",
    "loss_tr = 0\n",
    "loss_te = 0\n",
    "loss_tr_tmp = []\n",
    "loss_te_tmp = []\n",
    "ws = []\n",
    "for k in range(k_fold):\n",
    "    w, loss_tr_tmp_val, loss_te_tmp_val = cross_validation(y_final, X_final, k_indices, k, lambda_, gamma)\n",
    "    loss_tr_tmp.append(loss_tr_tmp_val)\n",
    "    loss_te_tmp.append(loss_te_tmp_val)\n",
    "    ws.append(w)\n",
    "loss_tr = np.mean(loss_tr_tmp)\n",
    "loss_te = np.mean(loss_te_tmp)\n",
    "print(ws)\n",
    "# Stack the arrays along a new axis (axis=2) to align components\n",
    "stacked_arrays = np.stack(ws, axis=2)\n",
    "# Calculate the mean along the third axis (axis=2)\n",
    "w_final = np.mean(stacked_arrays, axis=2)\n",
    "print(w_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to use our w to predict the results: we first format our x_test values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[22.3         2.          1.          4.          2.        ]\n",
      " [21.93        1.          2.          4.          2.        ]\n",
      " [28.03849189  2.          2.          4.          2.        ]\n",
      " ...\n",
      " [24.8         1.          2.          3.          2.        ]\n",
      " [23.99        1.          1.58073189  3.          2.        ]\n",
      " [42.18        2.          1.          4.          2.        ]]\n",
      "[[ 0.02388678]\n",
      " [ 0.03411269]\n",
      " [ 0.65128871]\n",
      " [-0.42831283]\n",
      " [-0.21467387]\n",
      " [-0.51024742]]\n"
     ]
    }
   ],
   "source": [
    "def extract_feature_test(name):\n",
    "    filename = 'x_test.csv'\n",
    "    first_line = np.array(read_first_line(filename))\n",
    "    index = np.where(first_line == name)\n",
    "    ind = index[0].item()\n",
    "    return x_test[:, ind-1]\n",
    "\n",
    "_BMI5_test = extract_feature_test('_BMI5')\n",
    "_BMI5_transformed_test = replace_mean(_BMI5_test)\n",
    "\n",
    "############### High blood pressure (9 = missing)\n",
    "_RFHYPE5_test = extract_feature_test('_RFHYPE5')\n",
    "_RFHYPE5_transformed_test = replace_mean(_RFHYPE5_test)\n",
    "_RFHYPE5_transformed_test = replace_with_mean_excluding_values(_RFHYPE5_test, np.array([9]))\n",
    "\n",
    "################ High cholesterol\n",
    "TOLDHI2_test = extract_feature_test('TOLDHI2')\n",
    "TOLDHI2_transformed_test = replace_mean(TOLDHI2_test)\n",
    "TOLDHI2_transformed_test = replace_with_mean_excluding_values(TOLDHI2_test, np.array([7, 9]))\n",
    "\n",
    "################ Smoking\n",
    "_SMOKER3_test = extract_feature_test('_SMOKER3')\n",
    "_SMOKER3_transformed_test = replace_mean(_SMOKER3_test)\n",
    "_SMOKER3_transformed_test = replace_with_mean_excluding_values(_SMOKER3_test, np.array([7, 9]))\n",
    "\n",
    "################ Has ever had a stroke\n",
    "CVDSTRK3_test = extract_feature_test('CVDSTRK3')\n",
    "CVDSTRK3_transformed_test = replace_mean(CVDSTRK3_test)\n",
    "CVDSTRK3_transformed_test = replace_with_mean_excluding_values(CVDSTRK3_test, np.array([7, 9]))\n",
    "\n",
    "X_test = np.hstack((_BMI5_transformed_test.reshape(-1, 1), _RFHYPE5_transformed_test.reshape(-1, 1), TOLDHI2_transformed_test.reshape(-1, 1), _SMOKER3_transformed_test.reshape(-1, 1), CVDSTRK3_transformed_test.reshape(-1, 1)))\n",
    "print(X_test)\n",
    "print(w_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we actually want to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(tx_test, w):\n",
    "    compute = sigmoid(np.dot(tx_test, w))\n",
    "    y_test = (compute >= 0.5).astype(int)\n",
    "    return y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.         22.3         2.          1.          4.          2.        ]\n",
      " [ 1.         21.93        1.          2.          4.          2.        ]\n",
      " [ 1.         28.03849189  2.          2.          4.          2.        ]\n",
      " ...\n",
      " [ 1.         24.8         1.          2.          3.          2.        ]\n",
      " [ 1.         23.99        1.          1.58073189  3.          2.        ]\n",
      " [ 1.         42.18        2.          1.          4.          2.        ]]\n",
      "[[ 0.02388678]\n",
      " [ 0.03411269]\n",
      " [ 0.65128871]\n",
      " [-0.42831283]\n",
      " [-0.21467387]\n",
      " [-0.51024742]]\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " ...\n",
      " [0]\n",
      " [0]\n",
      " [1]]\n"
     ]
    }
   ],
   "source": [
    "tx_test = np.c_[np.ones((X_test.shape[0], 1)), X_test]\n",
    "print(tx_test)\n",
    "print(w_final)\n",
    "y_pred = prediction(tx_test, w_final)\n",
    "print(y_pred)\n",
    "y_pred[y_pred == 0] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import create_csv_submission\n",
    "create_csv_submission(test_ids, y_pred, \"Submission_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
